---
layout:     post
title:      "NLP"
subtitle:   "coursera note"
date:       2020-11-12 20:00:00
author:     "Becks"
header-img: "img/post-bg2.jpg"
catalog:    true
tags:
    - Deep Learning
    - Machine Learning
    - 学习笔记
---

<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>

## Natural Language Processing with Classification and Vector Spaces

#### Week1 Vocabulary & Feature Extraction

**Sparse Representation**: 

- Represent Text as a vector. Check every word from vocabulary, if appear, note 1 else 0
- Disadvantage: Large
- Logistric regression learn **N+1** where N is the size of vocabulary
- Build vocabulary: process one by one and get sets of lists of word


```
e.g. I am happy because I am learning NLP

[I, am, happy, because, learning NLP, ... hated, the, movie]
 1   1   1       1         1       1        0     0     0

```



![](/img/post/Natural-Language-Processing/course1/week1pic1.png)

**Positive & Negative Frequencies**: Build vocabulary, map word to frequencey

![](/img/post/Natural-Language-Processing/course1/week1pic2.png)

![](/img/post/Natural-Language-Processing/course1/week1pic3.png)

![](/img/post/Natural-Language-Processing/course1/week1pic4.png)

![](/img/post/Natural-Language-Processing/course1/week1pic5.png)


<span style="background-color: #FFFF00">Faster speed for logistic regression, Instead learning V feature(size of vocabulary), only learn 3 features</span>


![](/img/post/Natural-Language-Processing/course1/week1pic6.png)

![](/img/post/Natural-Language-Processing/course1/week1pic7.png)

![](/img/post/Natural-Language-Processing/course1/week1pic8.png)


Hence you end up getting the following feature vector ```[1,8,11][1,8,11]```. 11 corresponds to the bias, 88 the positive feature, and 1111 the negative feature.


**Preprocessing**: 

When preprocessing, you have to perform the following:

1. Eliminate handles(@...) and URLs
2. Tokenize the string into words.
3. Remove stop words like  “and, is, are, at, has, for ,a”
4. Stemming- or convert every word to its stem. Like dancer, dancing, danced, becomes 'danc'. You can use porter stemmer to take care of this.
5. Convert all your words to lower case.


![](/img/post/Natural-Language-Processing/course1/week1pic9.png)


![](/img/post/Natural-Language-Processing/course1/week1pic10.png)

matrix with m rows and three columns. Every row: the feature for each tweets

![](/img/post/Natural-Language-Processing/course1/week1pic11.png)


**General Implementation**: 

```python
freqs = build_freqs(tweets,labels) #Build frequencies dictionary
X = np.zeros((m,3)) #Initialize matrix X
for i in range(m): #For every tweet
    p_tweet = process_tweet(tweets[i]) #process tweet, include
                                      # deleting stops, stemming, deleting URLS, and handles and lower cases
    X[i, :] = extract_features(p_tweet, freqs) #Extract Features: 
                                               #By summing up the positive and negative frequencies of the tweets
                                               
```

**Logistic Regression**

$$x^{\left( i \right)} $$ is i-th tweet

![](/img/post/Natural-Language-Processing/course1/week1pic12.png)

![](/img/post/Natural-Language-Processing/course1/week1pic13.png)


**Logistic Cost Function Derivation**

$$P\left( y \mid x^{\left( i \right)}, \theta \right) = h \left(x^{\left( i \right)}, \theta \right)^{y^\left( i \right)} \left( 1-  h \left(x^{\left( i \right)}, \theta \right) \right)^{\left( 1- y^\left( i \right) \right)} $$

when y = 1, you get $$h \left(x^{\left( i \right)}, \theta \right)$$, when y = 0, you get $$ \left( 1-  h \left(x^{\left( i \right)}, \theta \right) \right) $$. In either case, you want to maximize $$ h \left(x^{\left( i \right)}, \theta \right)$$ clsoe to 1. when y = 0, you want $$ \left( 1-  h \left(x^{\left( i \right)}, \theta \right) \right)  $$ to be 0  as $$ h \left(x^{\left( i \right)}, \theta \right)$$ close to 1, when y = 1, you want $$ h \left(x^{\left( i \right)}, \theta \right) = 1$$

Define the likelihood as 

$$L\left(  \theta \right) = \prod_{i=1}^{m}  h \left(\theta, x^{\left( i \right)} \right)^{y^\left( i \right)} \left( 1-  h \left(\theta, x^{\left( i \right)} \right) \right)^{\left( 1- y^\left( i \right) \right)}   $$


![](/img/post/Natural-Language-Processing/course1/week1pic14.png)


**Gradient Descent Derivation**

![](/img/post/Natural-Language-Processing/course1/week1pic15.png)

![](/img/post/Natural-Language-Processing/course1/week1pic16.png)

![](/img/post/Natural-Language-Processing/course1/week1pic17.png)


#### Week2 Naive Bayes

Tweet <span style="color:red">defined either positive or negative, cannot be both</span>. Bayes' rule is based on the mathmatical formulation of conditional probabilities. <span style="background-color: #FFFF00">Called **Naive** because make assumpton that the **features you're using for classification are all independent**, which in reality is rare</span>

$$ P\left(X \mid Y \right) = \frac{ P\left(Y \mid X \right) P\left(X \right)}{P\left(Y \right)} $$

e.g. Happy sometimes as positive word sometimes as negative, A the number of positive tweet, B the number tweet contain happy

![](/img/post/Natural-Language-Processing/course1/week2pic1.png)

$$ P\left(Positive \mid happy \right) = \frac{ P\left(Positive \cap happy \right)}{P\left(happy \right)} $$

$$ P\left(happy \mid Positive \right) = \frac{ P\left(Positive \cap happy \right)}{P\left(Positive \right)} $$

$$ P\left(Positive \mid happy \right) =  P\left(happy \mid Positive \right) \frac{ P\left(Positive \right)}{P\left(happy \right)} $$

Q: Suppose that in your dataset, 25% of the positive tweets contain the word ‘happy’. You also know that a total of 13% of the tweets in your dataset contain the word 'happy', and that 40% of the total number of tweets are positive. You observe the tweet: ''happy to learn NLP'. What is the probability that this tweet is positive? <br/>
A: 0.77

![](/img/post/Natural-Language-Processing/course1/week2pic2.png)

- Word with equal probability for positive/negative don't add anything to the sentiment (e.g. I, am, learning, NLP) 
- "sad, happy, not" have a significant difference between probabilities. 
- "because" negative class is zero, no way of comparing between two corpora, a problem

![](/img/post/Natural-Language-Processing/course1/week2pic3.png)


Once you have the probabilities, you can compute the likelihood score as follows. A score greater than 1 indicates that the class is positive, otherwise it is negative.


![](/img/post/Natural-Language-Processing/course1/week2pic4.png)


**Laplacian Smoothing**

We usually compute the probability of a word given a class as follows:

$$ P\left(w_i \mid class \right) = \frac{freq\left(wi,class \right)}{N_{class}} \quad \text{class} \in \text{\{Positive, Negative\}}$$

$$N_{positive}$$ is the total number of positive words, $$N_{negative}$$ is the number of negative words


However, if a word does not appear in the training, then it automatically gets a probability of 0, to fix this we add smoothing as follows

$$ P\left(w_i \mid positive \right) = \frac{freq\left(wi,positive \right) + 1}{N_{positive} + V}$$

$$ P\left(w_i \mid negative \right) = \frac{freq\left(wi,negative \right) + 1}{N_{negative} + V}$$

Note that we added all in the numerator, and since there are V words to normalize, we add V in the denominator. where 

$$N_{class}$$: count of all words in class,  <span style="color:red">if appear more than once, count all</span><br/>
$$V$$: number of unique words in vocabulary


![](/img/post/Natural-Language-Processing/course1/week2pic5.png)

**Log Likelihood**

![](/img/post/Natural-Language-Processing/course1/week2pic6.png)


To compute the log likelihood, we need to get the ratios and use them to compute a score that will allow us to decide whether a tweet is positive or negative.  <span style="background-color: #FFFF00">The higher the ratio, the more positive the word is</span>. To do inference, you can compute the following:



$$ \frac{P\left(pos\right)}{P\left(neg\right)} \prod_{i=1}^m \frac{P\left(w_i \mid pos\right)}{P\left(w_i \mid neg\right)} > 1 $$



m gets larger, we can get numerical flow issues, hard to store on device , so we introduce the log, which gives you the following equation 

$$ log\left( \frac{P\left( D_{pos}\right)}{P\left(D_{neg}\right)} \prod_{i=1}^m \frac{P\left(w_i \mid pos\right)}{P\left(w_i \mid neg\right)} \right) =>  \underbrace{log\frac{P\left(D_{pos}\right)}{P\left(D_{neg}\right)}}_{\text{log prior}} + \underbrace{\sum_{i=1}^m log \frac{P\left(w_i \mid pos\right)}{P\left(w_i \mid neg\right)}}_{\text{log likelihood}} $$

$$\text{where} \quad P\left(D_{pos}\right) = \frac{D_{pos}}{D},  P\left(D_{neg}\right) = \frac{D_{neg}}{D}$$
 
 <span style="background-color: #FFFF00">where D is the total number of documents, $$D_{pos}$$ is the total number of positive tweets and $$D_{neg}$$ is the total number of negative tweets.</span> 注：与$$N_{pos}$$ 不同， $$N_{pos}$$ 是total number of all positive words
 
<span style="background-color: #FFFF00">log prior is important for unbalanced dataset</span>. We further introduce λ as follows:

![](/img/post/Natural-Language-Processing/course1/week2pic7.png)

<span style="color:red">The positive value indicates tweet is positive. Negative value indicates tweet is negative</span>

![](/img/post/Natural-Language-Processing/course1/week2pic8.png)

**Training Naive Bayes**

1. collect and annotate corpus: divide tweets into two groups, positive/negative
2. Preprocess the tweets: process_tweet(tweet) ➞ [w1, w2, w3, ...]:
   - Lowercase
   - Remove punctuation, urls, names
   - Remove stop words
   - Stemming
   - Tokenize sentences
3. Compute freq(w, class), use below table to compute
4. Get $$P\left(w \mid  pos \right), P\left(w \mid neg \right)$$ using Laplacian Smoothing formula
5. Get $$\lambda\left( w \right) =  log \frac{P\left( w \mid pos \right)}{P\left( w \mid neg \right)}$$
6. Compute $$logprior = log \frac{D_{pos}}{D_{neg}} $$ where $$ D_{pos} $$ and $$D_{neg}$$ correspond to  the number of positive and negative documents respectively


![](/img/post/Natural-Language-Processing/course1/week2pic9.png)


**Testing Naive Bayes**

if a word not in vocabulary, think as neutral, not contribute to score

![](/img/post/Natural-Language-Processing/course1/week2pic10.png)

$$\text{Accuracy} \rightarrow \frac{1}{m} \sum_{i=1}^{m} {pred_i == Y_{val}} $$


**Naive Bayes Application**

There are many applications of naive Bayes including:

- Author identification: document written by which author, calculate $$\lambda$$ for each word for both author to build vocabulary
- Spam filtering: $$\frac{P\left(spam \mid email \right)}{P\left(nnspam \mid email \right)}$$
- Information retrieval: filter relevant and irrelevant documents in a database, calculate the likelihood of the documents given the query, retrieve document if $$P\left(document_k \mid query \right) \approx \prod_{i=0}^{\mid query \mid} P\left(query_i \mid document_k  \right) > threshold$$
- Word disambiguation: e.g. don't know bank in reading, refer to river or money? to calculate the score of the document, $$\frac{P\left(river \mid text\right)}{P\left(money \mid text\right)}$$. if refer to river not money, score is bigger than 1. 


**Naive Bayes Assummtion**

<span style="background-color: #FFFF00">Advantage: simple doesn't require setting any custom parameters</span>.

Assumption:

1. <span style="background-color: #FFFF00">Indepdence between predictors or features associated with each class</span>
  - assume word in a piece of text are independent. e.g. "It is sunny and hot in the Sahara desert." But "sunny" and "hot" often appear together. And together often related to beach or desert. Not always independent 
  - under or over estimate conditional probabilities of individual words
  - e.g. "It's always cold and snowy in ___" naive model will assign equal weight to the words "spring, summer, fall, winter".
2. Relative frequencies in corpus: <span style="background-color: #FFFF00">rely on distribution on training dataset</span>
   - A good dataset contain the same proportion of positive and negative weets as a random sample. But in reality positive tweet is more common(because negative tweets maybe muted or banned by twitter). This would result a very optimistic or pessimistic model


**Error Analysis**

1. Semantic meaning lost in the pre-processing step by removing punctuation and stop words
   - "My beloved grandmother :(", with punctuation indicating a sad face, after process, [belov, grandmoth], positive tweet
   - "This is not good, because your attitude is not even close to being nice." if remove neutral word, [good, attitude, close, nice] positive
2. Word order affect meaning
   - "I am happy because I did not go." vs. "I am not happy because I did go."
3. Adversarial attack: some quirks of languages come naturally to human but confuse Naive Bayes model
   - some common language phenomenon, Sarcasm, Irony and Euphemisms.
   - e.g. "This is a ridiculously powerful movie. The plot was gripping and I cried right through until the ending" positive. After process. [ridicul, power, movi, plot, grip, cry, end] negative