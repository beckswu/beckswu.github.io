---
layout:     post
title:      "NLP"
subtitle:   "coursera note"
date:       2020-11-12 20:00:00
author:     "Becks"
header-img: "img/post-bg2.jpg"
catalog:    true
tags:
    - Deep Learning
    - Machine Learning
    - Â≠¶‰π†Á¨îËÆ∞
---

<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>


## 1.1 Vocabulary & Feature Extraction

#### Sparse Representation

- Represent Text as a vector. Check every word from vocabulary, if appear, note 1 else 0
- Disadvantage: Large
- Logistric regression learn **N+1** where N is the size of vocabulary
- Build vocabulary: process one by one and get sets of lists of word


```
e.g. I am happy because I am learning NLP

[I, am, happy, because, learning NLP, ... hated, the, movie]
 1   1   1       1         1       1        0     0     0

```



![](/img/post/Natural-Language-Processing/course1/week1pic1.png)

**Positive & Negative Frequencies**: Build vocabulary, map word to frequencey

![](/img/post/Natural-Language-Processing/course1/week1pic2.gif)



<span style="background-color: #FFFF00">Faster speed for logistic regression, Instead learning V feature(size of vocabulary), only learn 3 features</span>


$$X_m = \left[ 1, \sum_{w_{pos}} freqs\left( w_{pos}, 1 \right),  \sum_{w_{neg}} freqs\left( w_{neg}, 1 \right) \right]$$

![](/img/post/Natural-Language-Processing/course1/week1pic6.gif)



Hence you end up getting the following feature vector ```[1,8,11]```. 11 corresponds to the bias, 88 the positive feature, and 11 the negative feature.


#### Preprocessing 

When preprocessing, you have to perform the following:

1. Eliminate handles(@...) and URLs
2. Tokenize the string into words.
3. Remove stop words like  ‚Äúand, is, are, at, has, for ,a‚Äù: Stop words are words that don't add significant meaning to the text.
4. Stemming- or convert every word to its stem. Like dancer, dancing, danced, becomes 'danc'. You can use porter stemmer to take care of this.
5. Convert all your words to lower case.
  
[example](https://github.com/beckswu/Natural-Language-Processing/tree/master/Coursera/Course1-Natural%20Language%20Processing%20with%20Classification%20and%20Vector%20Spaces)


![](/img/post/Natural-Language-Processing/course1/week1pic9.gif)


matrix with m rows and three columns. Every row: the feature for each tweets

![](/img/post/Natural-Language-Processing/course1/week1pic11.png)


**General Implementation**: 

```python
freqs = build_freqs(tweets,labels) #Build frequencies dictionary

X = np.zeros((m,3)) #Initialize matrix X

for i in range(m): #For every tweet

    p_tweet = process_tweet(tweets[i]) #process tweet, include

                                      # deleting stops, stemming, deleting URLS, and handles and lower cases

    X[i, :] = extract_features(p_tweet, freqs) #Extract Features: 

                                               #By summing up the positive and negative frequencies of the tweets
                                               
```

#### Logistic Regression

$$x^{\left( i \right)} $$ is i-th tweet

![](/img/post/Natural-Language-Processing/course1/week1pic12.png)

![](/img/post/Natural-Language-Processing/course1/week1pic13.png)


**Logistic Cost Function Derivation**

$$P\left( y \mid x^{\left( i \right)}, \theta \right) = h \left(x^{\left( i \right)}, \theta \right)^{y^\left( i \right)} \left( 1-  h \left(x^{\left( i \right)}, \theta \right) \right)^{\left( 1- y^\left( i \right) \right)} $$

when y = 1, you get $$h \left(x^{\left( i \right)}, \theta \right)$$, when y = 0, you get $$ \left( 1-  h \left(x^{\left( i \right)}, \theta \right) \right) $$. In either case, you want to maximize $$ h \left(x^{\left( i \right)}, \theta \right)$$ clsoe to 1. when y = 0, you want $$ \left( 1-  h \left(x^{\left( i \right)}, \theta \right) \right)  $$ to be 0  as $$ h \left(x^{\left( i \right)}, \theta \right)$$ close to 1, when y = 1, you want $$ h \left(x^{\left( i \right)}, \theta \right) = 1$$

Define the likelihood as 

$$L\left(  \theta \right) = \prod_{i=1}^{m}  h \left(\theta, x^{\left( i \right)} \right)^{y^\left( i \right)} \left( 1-  h \left(\theta, x^{\left( i \right)} \right) \right)^{\left( 1- y^\left( i \right) \right)}   $$


![](/img/post/Natural-Language-Processing/course1/week1pic14.png)


**Gradient Descent Derivation**

![](/img/post/Natural-Language-Processing/course1/week1pic15.png)

![](/img/post/Natural-Language-Processing/course1/week1pic16.png)

![](/img/post/Natural-Language-Processing/course1/week1pic17.png)



------------

if X is M x N matrix, N is the number of features 


$$ d\theta = \frac{1}{m} X^T \left( h\left(X, \theta \right) - y \right)$$

------------

<br/><br/><br/>


##  1.2 Naive Bayes

Tweet <span style="color:red">defined either positive or negative, cannot be both</span>. Bayes' rule is based on the mathmatical formulation of conditional probabilities. <span style="background-color: #FFFF00">Called **Naive** because make assumpton that the **features you're using for classification are all independent**, which in reality is rare</span>

$$ P\left(X \mid Y \right) = \frac{ P\left(Y \mid X \right) P\left(X \right)}{P\left(Y \right)} $$

e.g. Happy sometimes as positive word sometimes as negative, A the number of positive tweet, B the number tweet contain happy

![](/img/post/Natural-Language-Processing/course1/week2pic1.png)

$$ P\left(Positive \mid happy \right) = \frac{ P\left(Positive \cap happy \right)}{P\left(happy \right)} $$

$$ P\left(happy \mid Positive \right) = \frac{ P\left(Positive \cap happy \right)}{P\left(Positive \right)} $$

$$ P\left(Positive \mid happy \right) =  P\left(happy \mid Positive \right) \frac{ P\left(Positive \right)}{P\left(happy \right)} $$

e.g. Q: Suppose that in your dataset, 25% of the positive tweets contain the word ‚Äòhappy‚Äô. You also know that a total of 13% of the tweets in your dataset contain the word 'happy', and that 40% of the total number of tweets are positive. You observe the tweet: ''happy to learn NLP'. What is the probability that this tweet is positive? <br/>
A: 0.77

![](/img/post/Natural-Language-Processing/course1/week2pic2.png)

- Word with equal probability for positive/negative don't add anything to the sentiment (e.g. I, am, learning, NLP) 
- "sad, happy, not" have a significant difference between probabilities. 
- <span style="color:red">Problem: "because" negative class is zero, no way of comparing between two corpora</span>

![](/img/post/Natural-Language-Processing/course1/week2pic3.png)


Once you have the probabilities, you can compute the likelihood score as follows.<span style="background-color:#FFFF00"> A score greater than 1 indicates that the class is positive, otherwise it is negative.</span>


![](/img/post/Natural-Language-Processing/course1/week2pic4.png)


#### Laplacian Smoothing

We usually compute the probability of a word given a class as follows:

$$ P\left(w_i \mid class \right) = \frac{freq\left(wi,class \right)}{N_{class}} \quad \text{class} \in \text{\{Positive, Negative\}}$$

- $$freq_{positive}$$ and $$freq_{negative}$$  are the frequencies of that specific word in the positive or negative class. In other words, the positive frequency of a word is the number of times the word is counted with the label of 1.
- $$N_{positive}$$ and $$N_{negative}$$  are the total number of positive and negative words for all documents (for all tweets), respectively.


However, if a word does not appear in the training, then it automatically gets a probability of 0, to fix this we add smoothing as follows

$$ P\left(w_i \mid positive \right) = \frac{freq\left(wi,positive \right) + 1}{N_{positive} + V}$$

$$ P\left(w_i \mid negative \right) = \frac{freq\left(wi,negative \right) + 1}{N_{negative} + V}$$

where  <span style="color:red">$$V$$:  the number of unique words in the entire set of documents(ËÄå‰∏çÊòØtotal count of word), for all classes, whether positive or negative.</span>


![](/img/post/Natural-Language-Processing/course1/week2pic5.png)

**Log Likelihood**

![](/img/post/Natural-Language-Processing/course1/week2pic6.png)


To compute the log likelihood, we need to get the ratios and use them to compute a score that will allow us to decide whether a tweet is positive or negative.  <span style="background-color: #FFFF00">The higher the ratio, the more positive the word is</span>. To do inference, you can compute the following:



$$ \frac{P\left(pos\right)}{P\left(neg\right)} \prod_{i=1}^m \frac{P\left(w_i \mid pos\right)}{P\left(w_i \mid neg\right)} > 1 $$



<span style="color:red">m gets larger, we can get numerical flow issues,</span> hard to store on device , so we introduce the <span style="background-color:#FFFF00">**log**</span>, which gives you the following equation 

$$ log\left( \frac{P\left( D_{pos}\right)}{P\left(D_{neg}\right)} \prod_{i=1}^m \frac{P\left(w_i \mid pos\right)}{P\left(w_i \mid neg\right)} \right) =>  \underbrace{log\frac{P\left(D_{pos}\right)}{P\left(D_{neg}\right)}}_{\text{log prior}} + \underbrace{\sum_{i=1}^m log \frac{P\left(w_i \mid pos\right)}{P\left(w_i \mid neg\right)}}_{\text{log likelihood}} $$

$$\text{where} \quad P\left(D_{pos}\right) = \frac{D_{pos}}{D},  P\left(D_{neg}\right) = \frac{D_{neg}}{D}$$
 
 - Reason to use log prior ratio: in real world, number of positive tweets may not equal to the number of negative tweets in your training data 
 - <span style="background-color: #FFFF00">D is the total number of tweets </span> 
 - <span style="background-color: #FFFF00">$$D_{pos}$$ is the total number of positive tweets and $$D_{neg}$$ is the total number of negative tweets.</span> 
 - <span style="color:red">**Ê≥®Ôºö‰∏é$$N_{pos}$$ ‰∏çÂêåÔºå $$N_{pos}$$ ÊòØtotal number of all positive words**</span>
 
<span style="background-color: #FFFF00">log prior represents the underlying probability in the target population that a tweet is positive versus negative.  important for unbalanced dataset</span>. We further introduce Œª as follows:

![](/img/post/Natural-Language-Processing/course1/week2pic7.png)

<span style="color:red">The positive value indicates tweet is positive. Negative value indicates tweet is negative</span>

![](/img/post/Natural-Language-Processing/course1/week2pic8.png)

#### Training Naive Bayes

1. collect and annotate corpus: divide tweets into two groups, positive/negative
2. Preprocess the tweets: process_tweet(tweet) ‚ûû [w1, w2, w3, ...]:
   - Lowercase
   - Remove punctuation, urls, names
   - Remove stop words: Stop words are words that don't add significant meaning to the text. 
   - Stemming
   - Tokenize sentences
3. Compute freq(w, class), use below table to compute
4. Get $$P\left(w \mid  pos \right), P\left(w \mid neg \right)$$ using Laplacian Smoothing formula
5. Get $$\lambda\left( w \right) =  log \frac{P\left( w \mid pos \right)}{P\left( w \mid neg \right)}$$
6. Compute $$logprior = log \frac{D_{pos}}{D_{neg}} $$ where $$ D_{pos} $$ and $$D_{neg}$$ correspond to  the number of positive and negative documents respectively


![](/img/post/Natural-Language-Processing/course1/week2pic9.png)


**Testing Naive Bayes**

if a word not in vocabulary, think as neutral, not contribute to score

![](/img/post/Natural-Language-Processing/course1/week2pic10.png)

$$\text{Accuracy} \rightarrow \frac{1}{m} \sum_{i=1}^{m} {pred_i == Y_{val}} $$


#### Naive Bayes Application

There are many applications of naive Bayes including:

- Author identification: document written by which author, calculate $$\lambda$$ for each word for both author to build vocabulary
- Spam filtering: $$\frac{P\left(spam \mid email \right)}{P\left(nnspam \mid email \right)}$$
- Information retrieval: filter relevant and irrelevant documents in a database, calculate the likelihood of the documents given the query, retrieve document if $$P\left(document_k \mid query \right) \approx \prod_{i=0}^{\mid query \mid} P\left(query_i \mid document_k  \right) > threshold$$
- Word disambiguation: e.g. don't know bank in reading, refer to river or money? to calculate the score of the document, $$\frac{P\left(river \mid text\right)}{P\left(money \mid text\right)}$$. if refer to river not money, score is bigger than 1. 


#### Naive Bayes Assumption

<span style="background-color: #FFFF00">Advantage: simple doesn't require setting any custom parameters</span>.

Assumption:

1. <span style="background-color: #FFFF00">Indepdence between predictors or features associated with each class</span>
  - assume word in a piece of text are independent. e.g. "It is sunny and hot in the Sahara desert." But "sunny" and "hot" often appear together. And together often related to beach or desert. Not always independent 
  - <span style="color:red">Under or over estimate conditional probabilities of individual words</span> e.g. "It's always cold and snowy in ___" naive model will assign equal weight to the words "spring, summer, fall, winter".
2. Relative frequencies in corpus: <span style="background-color: #FFFF00">rely on distribution on training dataset</span>
   - A good dataset contain the same proportion of positive and negative weets as a random sample. But in reality positive tweet is more common(because negative tweets maybe muted or banned by twitter). This would result a very optimistic or pessimistic model


**Error Analysis**

1. Semantic meaning lost in the pre-processing step by removing punctuation and stop words
   - "My beloved grandmother :(", with punctuation indicating a sad face, after process, [belov, grandmoth], positive tweet
   - "This is not good, because your attitude is not even close to being nice." if remove neutral word, [good, attitude, close, nice] positive
2. Word order affect meaning
   - "I am happy because I did not go." vs. "I am not happy because I did go."
3. Adversarial attack: some quirks of languages come naturally to human but confuse Naive Bayes model
   - some common language phenomenon, Sarcasm, Irony and Euphemisms.
   - e.g. "This is a ridiculously powerful movie. The plot was gripping and I cried right through until the ending" positive. After process. [ridicul, power, movi, plot, grip, cry, end] negative


<br/><br/><br/>


## 1.3 Vector Space

e.g. "Where are you heading" vs "Where are you from" -> Different meaning Â∞ΩÁÆ°Ââç‰∏â‰∏™ËØç‰∏ÄÊ†∑<br/>
e.g. "What is your age" vs "How old are you" -> Same meaning

e.g. "You eat ceral from  a bowl" ceral and bowl are related <br/>
e.g. "You can buy something and someone else sells it" second half sentence is dependent on the first half

- Vector space: <span style="background-color: #FFFF00">identify the context around each word  to capture relative meaning, to represent words and documents as vector</span>
- Vector space allow to identify sentence are similar even if they do not share the same words
- Vector space allow to capture dependencies between words
- application:
  -  information extraction to answer questions in the style of who, what, where, how... 
  -  machine translation
  -  chatbots programming: Êú∫Âô®ÂÆ¢Êúç


#### Word by Word Design

number of times they <span style="background-color: #FFFF00">occur together within a certain **distance** k</span>


 <span style="background-color: #FFFF00">Co-occurence matrix</span>: e.g. data =2, count how many appearance for  each word has distance smaller or euqal than 2


![](/img/post/Natural-Language-Processing/course1/week3pic1.png)

<span style="color:red">Word by word , get n entries, with n between 1 and size(vocabulary)</span>



#### Word by Document Design

number of times they <span style="background-color: #FFFF00">occur together within a certain **category** k</span>

Divide corpus into different tops like Entertainment, Economy, Machine Learning. Below data appear 500 times in corpus related to Entertainment, 6620 times in documents related to Economy

![](/img/post/Natural-Language-Processing/course1/week3pic2.png)

![](/img/post/Natural-Language-Processing/course1/week3pic3.png)

#### Euclidean Distance

Euclidean Distance is the norm of the difference between vectors, straight line between points

$$d\left( \vec{v}, \vec{w} \right) = \sqrt{\sum_{i=1}^n \left(v_i - w_i \right)^2 } \quad \rightarrow \quad \text{Norm of }  \left( \vec{v}, \vec{w} \right) $$

![](/img/post/Natural-Language-Processing/course1/week3pic4.png)

```python
v = np.array([1,6,8])
w = np.array([0,4,6])

#Calculate the Eudlidean distance d 

d = np.linalg.norm(v-w)
```


#### Cosine Similarity

<span style="color:red">Problem with Euclidean distance</span>:  Vector space where corpora are represented by occurrence of the words "disease" and "eggs". Because Agriculture and history corpus has a similar number of words(Âá∫Áé∞ÂçïËØçÊï∞Èáè‰∏ÄÊ†∑), Suggests agriculture and history ÊØî agriculture and food Êõ¥ÊÉ≥Ëøë‰πâ

- if A and B are identical, you will get $$cos\left( \theta \right)=1$$
- If you get $$cos\left( \theta \right)=0$$, that means that they are orthogonal (or perpendicular).
- Numbers between 0 and 1 indicate a similarity score.
- Numbers between -1 and 0 indicate a dissimilarity score.
- <span style="background-color: #FFFF00">Advantage: Cosine similarity **isn't biased by the size** difference between the representations</span>

![](/img/post/Natural-Language-Processing/course1/week3pic5.png)

$$\begin{align} \hat{v} \cdot \hat{w} &= \Vert \hat{v} \Vert \Vert \hat{w} \Vert cos \left( \beta \right) \\  cos \left( \beta \right) &= \frac{\hat{v} \cdot \hat{w}}{\Vert \hat{v} \Vert \Vert \hat{w} \Vert}  \\ &= \frac{\left(20 \times 30\right) + \left(40 \times 20 \right)}{\sqrt{20^2 + 40^2} \times \sqrt{30^2 + 20^2}}\end{align}$$

-  proportional to the similarity between the directions of the vectors that you are comparing. 
-  the cosine similarity values between 0 and 1 for word by Word/Docs, cosine similarity values between -1 and 1 when using word embedding 

![](/img/post/Natural-Language-Processing/course1/week3pic6.png)

**PCA**

- PCA: dimenison reduction from high dimension to fewer dimension. project data into a lower dimension while retaining as much information as possible
- Original Space -> Uncorrelated features -> Dimension Reduction
- Can use visualization to check if PCA capure relationship among words
- <span style="color:red">**Eigenvector**</span>: <span style="background-color: #FFFF00">directions</span> of uncorrelated features for your data
- <span style="color:red">**Eigenvalue**</span>: amount of information retained by each feature(<span style="background-color: #FFFF00">variances</span> of data in each of those new features)
- <span style="color:red">**Dot Product**</span>: gives the <span style="background-color: #FFFF00">projection</span> on uncorrelated features

![](/img/post/Natural-Language-Processing/course1/week3pic7.png)

1. Mean Normalize Data  $$x_i = \frac{x_i - \mu_i}{\sigma_{x_{i}}}$$
2. Get Covariance Matrix
3. Perform Singular Value Decomposition to get set of three matrices(Eigenvectors on column wise, Eigenvalues on diagonal). <span style="background-color: #FFFF00"> Eigenvalues and eigenvectors should be organized according to the eigenvalues in descending order. Ensure to retain as much as information as possible </span>
4. Project data to a new set of features, using the eigenvectors and eigenvalues in this steps


![](/img/post/Natural-Language-Processing/course1/week3pic8.png)

![](/img/post/Natural-Language-Processing/course1/week3pic9.png)

[Sample Code for PCA](https://github.com/beckswu/Natural-Language-Processing/tree/master/Coursera/Course1-Natural%20Language%20Processing%20with%20Classification%20and%20Vector%20Spaces)

<br/><br/><br/>

## 1.4 Transforming word vectors

Translate word: calculate word embedding in English and French, transform word embedding in English to French and find the most similar one

![](/img/post/Natural-Language-Processing/course1/week4pic1.png)

R: transform matrix, X: English word, Y: French word.  First get subsets of English word and French equivalence

![](/img/post/Natural-Language-Processing/course1/week4pic2.png)

<span style="color:red">Why not store all vocabulary?</span>: just need a subset to collect to find transform matrix. If it works well, then model can translate words that are not in vocabulary

**Solving for R**:

$$ \begin{align} \color{olive}{\text{Initialize R}}\quad & \\
 \color{blue}{\text{in a loop:}}\quad & \\ 
 & Loss = \Vert XR - Y \Vert_F  \qquad   \color{fuchsia}{\text{Frobenium Norm}} \\
 & g = \frac{d}{dR} Loss \qquad   \color{fuchsia}{\text{gradient R}}  \\
 & R = R - \alpha g \qquad \quad   \color{fuchsia}{\text{update}} 
\end{align} $$ 


Can pick a fixed number of times to go through the loop or check the loss at each iteration and break out loop when loss falls between a certain threshold.


<span style="background-color: #FFFF00">Explanation for fixed number of iterations iterating until the loss falls below a threshold instead checking loss at each iteration.</span>

- You <span style="color:red">cannot rely on training loss getting low</span> -- what you really want is the validation loss to go down, or validation accuracy to go up. And indeed - in some cases people train until validation accuracy reaches a threshold, or -- commonly known as "early stopping" -- until the validation accuracy starts to go down, which is a sign of over-fitting.
- Why not always do "early stopping"? Well, mostly because <span style="color:red">well-regularized models on larger data-sets never stop improving</span>. Especially in NLP, you can often continue training for months and the model will continue getting slightly and slightly better. This is also the reason why it's hard to just stop at a threshold -- unless there's an external customer setting the threshold, why stop, where do you put the threshold?
- Stopping after a certain number of steps has the advantage that you know <span style="color:red">how long your training will take</span> - so you can keep some sanity and not train for months. You can then try to get the best performance within this time budget. <span style="color:red">Another advantage is that you can fix your learning rate schedule</span> -- e.g., lower the learning rate at 10% before finish, and then again more at 1% before finishing. Such learning rate schedules help a lot, but are harder to do if you don't know how long you're training.

#### Frobenius norm


$$ \Vert A \Vert_F = \sqrt{\sum_{i=1}^m \sum_{j=1}^n \vert a_{ij}\vert^2 } $$ 

e.g. has two word embedding and dimension is two, the matrix is 2x2

$$ A = \begin{pmatrix}
    2 & 2\\
    2 & 2\\
  \end{pmatrix}$$

$$  \Vert A \Vert_F = \sqrt{2^2 + 2^2 + 2^2 + 2^2} = 4 $$

```python
A = np.array([[2,2,],
              [2,2,]])
A_squared = np.square(A)
print(A_squared)  # array([[4,4,],

                  #        [4,4,]])

A_Frobenious = np.sqrt(np.sum(A_squared))
```

**Frobenius norm squared**

$$\frac{1}{m} \Vert XR - Y \Vert_F^2 $$

- The norm is always nonnegative (we're summing up absolute values), and so is the square.
- When we take the square of all non-negative (positive or zero) numbers, the order of the data is preserved. For example, if 3 > 2, 3^2 > 2^2
- Using the norm or squared norm in gradient descent <span style="color:red">results in the same location of the minimum</span>.
- Squaring cancels the square root in the Frobenius norm formula. Because of the chain rule for derivative, we would have to do more calculations if we had a square root in our expression for summation.
- Dividing the function value by the positive number doesn't change the optimum of the function, for the same reason as described above.
- divide by m: We're interested in transforming English embedding into the French. Thus, it is more important to <span style="background-color:#FFFF00">measure average loss per embedding</span> than the loss for the entire dictionary (which increases as the number of words in the dictionary increases).



so the gradient will be: 

**Gradient**

Easier to take derivative rather than dealing with the square root in Frobenius norm

$$  Loss = \Vert XR - Y \Vert_F^2  $$

$$  g = \frac{d}{dR}Loss = \frac{2}{m} X^T \left( XR - Y  \right) $$


#### Locality Sensitive Hashing

<span style="background-color: #FFFF00">Reduce computational cost(much faster) to find K-nearest neighbor </span>is to use locality sensitive hashing. For example, using word vectors with just two dimensions. **plan** can helps bucket vectors into subsets based on localtion(ËìùËâ≤ÁÇπÂú®ËìùËâ≤plane ‰∏äÊñπÔºåÁÅ∞Ëâ≤ÁÇπÂú®ÁÅ∞Ëâ≤plane‰∏äÊñπ)

![](/img/post/Natural-Language-Processing/course1/week4pic3.png)

normal vector: perpendicular to any vectors on the plane.

![](/img/post/Natural-Language-Processing/course1/week4pic4.png)

If dot product positive, it's on one side of the plane. If dot product negative, it's on opposite side of the plane. If dot product is zero, it is on plane

dot product of P and V1 is equal to the do length of P multiply V1 projection onto P

![](/img/post/Natural-Language-Processing/course1/week4pic5.png)

![](/img/post/Natural-Language-Processing/course1/week4pic6.png)

```python
def side_of_plane(P,v):
  dotprodcut = np.dot(P,v.T)
  sign_of_dot_product = np.sign(dotproduct)
  sign_of_dot_prodcut_scalar = np.asscalar(sign_of_dot_product)
  return sign_of_dot_prodcut_scalar
```

#### Multiple Plane

To divide vector space into manageable regions, want to use more than one plane. Get multiple signal, one for each plane, but need a single hash value 


![](/img/post/Natural-Language-Processing/course1/week4pic7.png)

Rule: 

$$  \color{maroon}{\text{sign_i} \geq 0, \rightarrow h_i = 1}  $$

$$  \color{maroon}{\text{sign_i} < 0, \rightarrow h_i = 0 } $$

$$  \mathbf{hash = \sum_i^H 2^i \times h_i} $$

```python
def hash_multiple_plane(P_l, v):
    hash_value = 0
    for i, P in enumerate(P_l):
        sign = side_of_plane(P,v)
        hash_i = 1 if sign >=0 else 0
        hash_value += 2**i * hash_i
    return hash_value
```



```python
#Generate Random Plane

num_dimensions = 2 
num_planes = 3

random_planes_matrix = np.random.normal(
                       size = (num_planes,
                               num_dimensions)) 
#[[ 1.76405235  0.40015721]

# [ 0.97873798  2.2408932 ]

# [ 1.86755799 -0.97727788]]

#find out whether vector v is on positive or negative side of each planes

def side_of_plane_matrix(P,v):
  dotproduct = np.dot(P,v.T)
  sign_of_dot_product = np.sign(dotproduct)
  return sign_of_dot_product

v = np.array([[2,2]])
num_planes_matrix = side_of_plane_matrix(random_planes_matrix, v)
#array([[1.],

#       [1.],

#       [1.]])

```


Text can be embedded into vector space so that nearest neighbors refer to text with similar meaning

![](/img/post/Natural-Language-Processing/course1/week4pic8.png)

<br/><br/><br/>

## 2.1 Autocorrect and Minimum Edit Distance


How it works?

1. Identify a misspelled word(only spelling error, not contextual error(e.g. Happy birthday to deer friend))   - how ? if spelled correctly, it is in dictionary. if not, probably a misspelled word
2. Find strings n edit distance way  e.g. deah vs dear
   - Edit: an operation perfomed on a string to change it, including: insert, delete, switch (eta - > eat, eta -> tea, but not eta -> ate), replace
   - Autocorrect: n is usually 1 to 3 edits
3. Filter candidates 
   - step 2 generate word if not in vocabulary, remove it. e.g. deah -> d_ar, if d_ar not in vocabulary remove it
4. Calculate word probabilities: $$ P\left( w \right) = \frac{C\left( w \right) }{V}$$, $$C\left( w \right)$$Âú®ÊñáÊú¨‰∏≠Âá∫Áé∞Ê¨°Êï∞, V: total size of the corpus

![](/img/post/Natural-Language-Processing/course2/week1pic1.png)

Minimum distance application: spelling correction, document similarity, machine translation, DNA sequencing etc

Minimum edit distance(**Levenshtein distance**): insert(cost 1), delete(cost 1), <span style="background-color: #FFFF00">replace(cost 2, can think of delete then insert)</span>

![](/img/post/Natural-Language-Processing/course2/week1pic2.png)

<br/><br/><br/>

## 2.2 Part of Speech Tagging and Hidden Markov Models

![](/img/post/Natural-Language-Processing/course2/week2pic1.png)

[Tag Meanings](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)

Application: 

- indentify entity: "Effel Tower is in Paris"  Effel Tower & Paris are entities
- Co-reference resolution: "Effel Tower is in Paris, it is 324 meters high", speech tagging indentify it refer to Eiffel Tower
- Speech Recognition: check if a sequence of word has a high probability or not


#### Markov chain

**Markov chain**: type of stochastic model that describes a sequence of possible events. To get probability of each event, it needs only the states of the previous events. Stochastic means random or randomness. Markov chain can be depicted as <span style="background-color: #FFFF00">directed graph</span>. A state refers to a certain condition of the present moment.

e.g. Why not learn ... (next is verb? or norm?) depends on previous word

![](/img/post/Natural-Language-Processing/course2/week2pic2.png)

To predict the first word, because state depend on previous state, <span style="background-color:#FFFF00">introduce initial state</span>. Transition Matrix A (n+1) x n where n is the number of hidden states.

![](/img/post/Natural-Language-Processing/course2/week2pic3.png)


#### Hidden Markov Model

For human, can recognize fly = verb, but machine cannot. Observable: can only seen by the machine

![](/img/post/Natural-Language-Processing/course2/week2pic5.png)


<span style="color:red">**Emission probability**</span>: describe the transition from hidden states of hidden Markov model(Norn, verb...) to the words of corpus(‰ªéÂçïËØçÁöÑËØçÊÄß Âà∞ ÂÖ∑‰ΩìÁöÑÂçïËØç)

![](/img/post/Natural-Language-Processing/course2/week2pic6.png)

e.g. VB -> eat 0.5 means:  when model is currently at hidden state for a verb, 0.5 chance that observable the model will emit the word eats.  ÂèØ‰ª•Ê≥®ÊÑèÂà∞ÊØè‰∏™word ÁöÑtagÁöÑÊ¶ÇÁéáÈÉΩÂ§ß‰∫é0ÔºåÂõ†‰∏∫context ‰∏çÂêåÔºåtag‰∏çÂêå, ‰æãÂ¶Ç He lay on his <span style="color:red">*back*</span> V.S. I'll be <span style="color:red">*back*</span>


![](/img/post/Natural-Language-Processing/course2/week2pic7.png)



![](/img/post/Natural-Language-Processing/course2/week2pic8.png)

#### Calculate Probability

Process Corpus:

1. <span style="background-color:#FFFF00">Add start token in each line of sentence as to calculate initial probability</span>
2. transform all words in lowercase and keep punctuation 

<span style="background-color:#FFFF00">**Transition Matrix**:</span>

![](/img/post/Natural-Language-Processing/course2/week2pic9.gif)



![](/img/post/Natural-Language-Processing/course2/week2pic9.png)





![](/img/post/Natural-Language-Processing/course2/week2pic10.gif)

#### Soothing 

- ‰∏∫‰∫ÜÈÅøÂÖçprobability as 0, add a small value eplison(e.g. 0.001)
-  vb is 1/3 for all outgoing transitions. ÊòØReasonableÁöÑ because don't have any data to estimate these probabilities
- In reall world, <span style="background-color:#FFFF00">**don't apply smoothing for initial probability**. Because: if add, allow a sentence to start with any parts of speech tag, including punctuation</span>


![](/img/post/Natural-Language-Processing/course2/week2pic11.gif)



<span style="background-color:#FFFF00">**Populating the Emission Matrix**</span>

![](/img/post/Natural-Language-Processing/course2/week2pic12.gif)


$$ C\left(t_i \right) $$: tag i ÊÄªÂÖ±Âá∫Áé∞ÁöÑÊ¨°Êï∞Ôºå<br/>
$$ C\left(t_i, w_i \right) $$ word_i Â±û‰∫étag_i Âá∫Áé∞ÁöÑÊ¨°Êï∞

![](/img/post/Natural-Language-Processing/course2/week2pic13.gif)


#### Viterbi Algorithm: a graph algorithm

Given a sequence of words, predict ÊØè‰∏™ËØçÂ±û‰∫é‰ªÄ‰πàtag, e.g. given corpus ```['see', 'them', 'here', 'with', 'us', '.']```,predict ```['VB', 'PRP', 'RB', 'IN', 'PRP', '.']```

‰∏ãÈù¢‰æãÂ≠ê:

- from O -> NN / VB has the same probability. But the emission probability of love is higher for VB.
- From love(VB) -> to(O), because Ê≤°ÊúâÂÖ∂‰ªñÁöÑtagÊúâemission probability of to

![](/img/post/Natural-Language-Processing/course2/week2pic15.gif)

Viterbi Algorithm ÈÄöËøá‰∏§‰∏™matrix (C&D) ÂÆûÁé∞

- Matrix C: hold ther intermediate optimal probabilities 
  - first column C: the probability <span style="color:red">from start state $$\pi$$ to the first tag $$t_i$$</span> and  word $$W_1$$. $$  c_{i,1} = \pi_i \times b_{i,cindex\left(w_1\right)} $$: product of transition probability of initial state (from initial state to tag i) and emission probability b(from tag i to word1)
- Matrix D: The most likely indices of previous visited state given word $$w_i$$
  - The column is all 0, no proceeding tags traversed
- Both matrix <span style="color:red">n rows: the number of speech tags/hidden states. K columns: the number of words in given sequences</span>

![](/img/post/Natural-Language-Processing/course2/week2pic16.gif)


**Forward Pass**

*Matrix C*: $$  c_{i,j} $$: Á¨¨i‰∏™tag, Á¨¨j‰∏™ÂçïËØç

$$  c_{i,j} = \underset{k}{max} \ c_{k,j-1} \times a_{k,i} \times  b_{i,cindex\left(w_j\right)} \text{}$$


- <span style="background-color: #FFFF00">$$b_{i,cindex\left(w_j\right)}$$: emission proability from $$tag_i$$ to $$word_j$$</span>
- <span style="background-color: #FFFF00">$$a_{k,i}$$ the transition probability from previous $$tag_k$$ to current $$tag_i$$</span>
- <span style="background-color: #FFFF00">$$c_{k,j-1} $$: the probability of proceeding path traversed</span> Âà∞ tag k, Á¨¨j-1 ‰∏™textÊúÄ‰Ω≥ÁöÑÊ¶ÇÁéá
- Then choose k to maximize the entire formula

In real word use <span style="color:red">**log probability**: to avoid multiply very small number, may lead to numeric issue for multiplication </span>

$$  c_{i,j} = \underset{k}{max} log\left(\ c_{k,j-1}\right) + log\left(a_{k,i}\right) +  log\left(b_{i,cindex\left(w_j\right)}\right) \text{}$$

*Matrix D*

$$  d_{i,j} = \underset{k}{argmax} \ c_{k,j-1} \times a_{k,i} \times  b_{i,cindex\left(w_j\right)} \text{}$$

- in each $$  d_{i,j}$$  save the k which maximize the entry in $$ c_{i,j}$$
- argmax returns k which maximize the function arguments instead of maximum value

In the below example, three states are not initial state, state can be 1,2,3. k can be 1,2,3


![](/img/post/Natural-Language-Processing/course2/week2pic17.gif)


**Backward Pass**

Extract the graph from matrix D, which represents most likely sequence of hidden states:  

1. Get the index of entry of $$C_{i,k}$$ which is the highest probability in the last column C. 
2. Use that index s ($$\underset{i}{argmax} \ c_{i,k}$$) to traverse backwards through the matrix D to reconstruct the sequence. ‰ªéÂêéÂæÄÂâçÊé®

e.g. ‰∏ãÈù¢ÁöÑ‰æãÂ≠ê,

1. ÊúÄÂêé‰∏ÄÂàóÁöÑÊúÄÂ§ßÁöÑÊ¶ÇÁéáÊòØ  $$C_{1,5} = 0.01$$, $$s = \underset{i}{argmax} \ c_{i,k} = 1$$. The most likely tag for word5 is tag1
2. $$D_{1,5} = 3$$, so previous index is 3 ( tag 3 is the pervious state with the highest proabability for $$C_{1,5}$$)
3. Algorithm stops as arrived at the start token

![](/img/post/Natural-Language-Processing/course2/week2pic18.gif)

[Forward & Backward Example](https://github.com/beckswu/Natural-Language-Processing/blob/master/Coursera/Course2-Natural%20Language%20Processing%20with%20Probabilistic%20Models/week2-Assignment-%20Part%20of%20Speech%20Tagging.ipynb)

<br/><br/><br/>

## 2.3 Autocomplete and Language Models

**Language model**: calculate the probability of a given sentence

- Estimate probability of word sequences. 
- Estimate probability of a word following a sequence of words with most likely suggestions(**Autocomplete a sentence**) e.g. if you type, how are, e-mail application guess you
  - It estimate the probability by splitting sentence into series of n-grams then find their probability in the probability matrix. Then language model predict next elements of sequence by return the word with <span style="color:red">highest probability</span>


Application:

- Speech recognition:  P(I saw a van) > P(eyes awe of an)
- Spelling correction
- Augmentatitve communication: predict most likely word from menu for people unable to physically talk or sign
  - Augmentative communication system: take a series of hand gestures from a user to help them form words and sentences


#### N-gram 

- An N-gram: sequence of N words. <span style="background-color:#FFFF00">Order matters</span>
- punctuation is treated as words. But all other special characters such as codes will be removed


E.g. 
- Bigrams are all sets of two words that appear side by side in corpus. <span style="color:red">Notes: ‰∏ãÈù¢‰æãÂ≠ê, ```I am``` appear twice, ‰ΩÜÊòØonly include ‰∏ÄÊ¨°Âú® bigram set</span>, I happy Âõ†‰∏∫‰∏çÁõ∏ÈÇªÔºåÊâÄ‰ª•‰∏çÊòØ
- Tigram: represent unique triplets of words that appear together in the sequence together in the corpus


$$ \text{Corpus: I am happy because I am learning} $$

$$ \text{Unigrams: \{I, am, happy, because,learning\}} $$

$$ \text{Bigrams: \{I am, am happy, happy because,...\}},note: \color{red}{\text{"I happy" not in bigram}  } $$

$$ \text{Bigrams: \{I am happy, am happy because, ...\}} $$

Sequence Notation:

$$W_1^3: \text{sequence of words from word 1 to word 3}$$

$$W_{m-2}^m: \text{sequence of wrods from word m-2 to word m}$$

![](/img/post/Natural-Language-Processing/course2/week3pic1.png)

#### Probability

Note: in bigram: $$ P\left( happy \mid I \right) = 0$$ Âõ†‰∏∫ I happy never appear in the corpus

$$ \text{Probabiity of bigram} \quad P\left( y \mid x \right) = \frac{ C\left(x, y \right)}{\sum_w{C\left(x, w \right)}} = \frac{ C\left(x y \right)}{C\left(x \right)}$$

$$\text{where }C_\left(x \right): \text{the count of all unigram x.} \quad \sum_w{C\left(x, w \right)}: \text{the count of all bigram starting with x}$$

$$\sum_w{C\left(x, w \right)}$$ bigram can be simpilied to $$C_\left(x \right)$$ unigram: <span style="color:red">**only works if x is followed by another word**</span>

$$ \text{Probabiity of trigram} \quad P\left( w_3 \mid w_1^2 \right) = \frac{ C\left(w_1^2 w_3\right)}{C\left(w_1^2\right)}$$

$$C\left(w_1^2 w_3\right) = C\left(w_1 w_2 w_3 \right) \quad \text{a bigram followed by a unigram}$$

$$ \text{Probabiity of N-gram} \quad P\left( w_N \mid w_1^{N-1} \right) = \frac{ C\left(w_1^{N-1} w_N\right)}{C\left(w_1^{N-1}\right)}$$

![](/img/post/Natural-Language-Processing/course2/week3pic2.gif)

$$ \text{Chain Rule} \quad P\left(A,B,C,D\right) = P\left(A \right)P\left(B \mid A \right)P\left( C \mid\ A,B \right)P\left(D\mid A, B, C \right)$$

$$P\left(the \ teacher \ drinks \ tea\right) = P\left(the \right)P\left(teacher \mid the \right)P\left( drinks \mid\ the, teacher \right)P\left(tea\mid the, teacher, drinks \right)$$

**Problem with above calculation**: <span style="background-color: #FFFF00">Corpus almost never contains the exact sentences we're interested in or even its longer subsequences</span>, ÊØîÂ¶Ç$$C\left( the \ teacher \ drinks \ tea \right)$$ and $$C\left( the \ teacher \ drinks  \right)$$ ÂèØËÉΩÈÉΩ‰∏ç‰ºöÂá∫Áé∞Âú® training corpus, the count are 0. <span style="color:red">ÂΩìsentences get longer and longer, the likelihood that more and more words will occur next to each other in the exact order gets smaller and smaller</span>



<span style="background-color: #FFFF00">Solution: use approximation: **product of probabilities of bigrams**</span>

$$P\left(the \ teacher \ drinks \ tea\right) \approx P\left(the \right)P\left(teacher \mid the \right)P\left( drinks \mid teacher \right)P\left(tea\mid drinks \right)$$


- Markove assumption: only last N words matter
- Bigram: $$P\left( w_n \mid w_1^{n-1} \right) \approx P\left( w_n \mid w_{n-1} \right) $$
- N-gram: $$P\left( w_n \mid w_1^{n-1} \right) \approx P\left( w_n \mid w_{n-N+1}^{n-1} \right) $$
- Entire sentence modeled with bigram: $$P\left( w_1^n \right) \approx \prod_{i=1}^{n} P\left( w_i \mid w_i-1 \right) $$
  - contrast with Naive Bayes where approximate sentence probability without cnsidering any word history
  - $$P\left( w_1^n \right) \approx P\left(w_1 \right)P\left(w_2 \mid w_1 \right)P ... P\left(w_n\mid w_{n-1} \right) $$: The first term of Bigram reiles on unigram probability of first word n sentence

![](/img/post/Natural-Language-Processing/course2/week3pic3.gif)


#### Start & End of Sentences

Start: 

- Problem: first word, don't have previous word, can't calculate bigram probability for first word
- Solution: add a special term ```<s>``` so can calculate the probabilities
- For n-grams, first N words, don't have enough context, <span style="background-color: #FFFF00">add n-1 special term</span>

![](/img/post/Natural-Language-Processing/course2/week3pic4.gif)

End: 

Problem 1:  $$ \text{Probabiity of bigram} \quad P\left( y \mid x \right) = \frac{ C\left(x, y \right)}{\sum_w{C\left(x, w \right)}} = \frac{ C\left(x y \right)}{C\left(x \right)}$$, $$\sum_w C\left(x, w \right)$$ the count of all bigram starting with X. <span style="color:red">When x is last word of sentences, doesn't work</span>

e.g. Corpus:

- ```<s> Lyn drinks chocolate```
- ```<s> John drinks```

$$\sum_w{C\left(drinks \ w \right)} = 1, C\left(drinks\right) = 2$$

Solution: add end of sentence token


Problem 2: 

e.g. Corpus:

- ```<s> yes no```
- ```<s> yes yes```
- ```<s> no no```

all possible sentences of lengths 2:  add all probability will equal to 1  üëå 

- ```<s> yes yes```,  $$P\left( <{s}> yes \ yes \right) = \frac{1}{3}$$
- ```<s> yes no```, $$P\left( <{s}> yes \ no \right) = \frac{1}{3}$$
- ```<s> no no```   $$P\left( <{s}> no \ no \right) = \frac{1}{3}$$
- ```<s> no yes``` $$P\left( <{s}> no \ yes \right) = 0$$

all possible sentences of lengths 3: add all probability will equal to 1 üëå 

- ```<s> yes yes yes```,  $$P\left( <{s}> yes \ yes \right) = ...$$
- ```<s> yes no no```, $$P\left( <{s}> yes \ no \right) = ...$$
- ```<s> no no no``` $$P\left( <{s}> no \ yes \right) = ...$$

However, What you want is the sum of all sentences of all length to 1: $$\sum_{2 word}P\left( ...\right) + \sum_{3 word}P\left( ...\right) ...  = 1$$

**Solution**: add end of sentence token ```</s>```  

Bigram:

- ```<s> Lyn drinks chocolate </s>```
- ```<s> John drinks </s>```

$$\sum_w{C\left(drinks \ w \right)} = 2, C\left(drinks\right) = 2$$


e.g. Bigram:

```<s> the teacher drinks tea``` => ```<s> the teacher drinks tea </s>``` 

$$P\left(the | <{s}> \right)P\left(teacher \mid the \right)P\left( drinks \mid teacher \right)P\left(tea\mid drinks \right)P\left(<{/s}>| tea \right)$$

e.g. Bigram:

- ```<s> Lyn drinks chocolate </s>``` $$P\left( sentence \right) = \frac{2}{3} * \frac{1}{2} * \frac{1}{2} * \frac{2}{2} = \frac{1}{6}$$
- ```<s> John drinks tea </s>```  $$P\left( sentence \right) = \frac{1}{3} * \frac{1}{1} * \frac{1}{2}* \frac{1}{1}  = \frac{1}{6}$$
- ```<s> Lyn eats chocolate </s>``` $$P\left( sentence \right) = \frac{2}{3} * \frac{1}{2} * \frac{1}{1} *  \frac{2}{2} = \frac{1}{3}$$

$$P\left( John \mid <{s}> \right) = \frac{1}{3},P\left( <{/s}> \mid tea  \right) = 1 $$

$$P\left( chocolate \mid eats \right) = \frac{1}{3},P\left( lyn \mid <{s}>  \right) = \frac{2}{3} $$



N-gram: => <span style="background-color: #FFFF00">just one ```</s>``` at the end of sentences</span>

e.g. Trigram:

```<s> <s> the teacher drinks tea </s>``` 

**Count Matrix & Probability Matrix**

$$\text{Note: } P\left( w_n \mid w_{n-N+1}^{n-1} \right) = \frac{ C\left(w_{n-N+1}^{n-1}, w_n\right)}{C\left( w_{n-N+1}^{n-1}\right)}, \text{w_n Ë°®Á§∫Á¨¨n‰∏™word}$$

- Count Matrix row: unique corpus (N-1)-grams
- Count Matrix columns: unique corpus words
- Probability Matrix: divde each cell from Count Matrix by Count Matrix row sum 
    - Count Matrix row sum equivalent to your counts of n-1 gram:  $$sum\left( row \right) = \sum_{w \in V} C\left(w_{n-N+1}^{n-1}, w\right) = C\left(w_{n-N+1}^{n-1}\right)$$



**Log probability**:  all probability in calculation <= 1 and mutiplying them brings risk of underflow

![](/img/post/Natural-Language-Processing/course2/week3pic5.gif)

#### Algorithm

1. Choose sentence start:  <span style="background-color: #FFFF00">**randomly choose** among all bigrams starting with ```<s>``` based on bigram probability. Bigrams with higher probabilities are more likely to be chosen.</span>
2. Choose next bigram starting with previous word from step 1 **randomly**, Ê¶ÇÁéáÊõ¥È´òÔºåË¢´ÊäΩÂà∞Ê¶ÇÁéáÊõ¥Â§ß
3. Continue until ```</s>``` is picked


e.g. :  choose ÊòØÁªøËâ≤

![](/img/post/Natural-Language-Processing/course2/week3pic6.png)


#### Perplexity

**Language Model Evaluation**

 - For smaller corpora: 80% Train, 10% Validation(used for tuning hyper-parameters), 10% Test
 - For large corpora(typical for text): 98% Train, 1% Validation(used for tuning hyper-parameters), 1% Test

![](/img/post/Natural-Language-Processing/course2/week3pic7.png)



$$\begin{align} PP\left(W \right) &= P\left(s_1,s_2,...,s_m \right)^{\frac{1}{m}} \\ &= \sqrt[m]{\prod_{i=1}^{m}  \frac{1}{ P\left( w_i \mid  w_{i-1}...w_{i-N+1} \right)   }} \\ &= \sqrt[m]{\prod_{i=1}^{m}  \frac{C\left(w_{i-1},... w_{i-N+1}, w_i \right)}{ C \left(w_{i-1},... w_{i-N+1}\right)   }} \end{align}$$left(W \right) =  \,\, =\color{red}{w_i \text{: i-th word in test set}}$$


- W : test set containing m sentence
- $$S_i$$ i-th sentence in the test set, each ending with ```</s>```
- m: number of all words in entire test set W <span style="color:red">including ```</s>``` but not including ```<s>```</span>
- Perplexity tells whether a set of sentences look like written by human or machine choosing words at random
- <span style="background-color: #FFFF00">A text written by human => low perplexity score. Text generated by random word choice => high perplexity score </span>
- Perplexity is the inverse probability of test sets normalized by <span style="color:red">the number of words</span> in the test set => higher the language model probability esitmates lower perplexity will be
- <span style="color:red">Perplexity is closely related to **entropy** which measures uncertainty</span>
- <span style="background-color: #FFFF00">Good language model have perplexity score between 60 to 20. Log perplexity is between 4.3 and 5.9 </span>
- perplexity for character level language lower perplexity for than word-based model
- The same probability for different test sets, <span style="background-color:#FFFF00">**the bigger m the lower final perplexity will be**</span>

e.g. 100 word in the text sets, m = 100, probability 0.9 which is high => means predict test sets very well

$$PP\left(W \right) = 0.9 => PP\left( W \right) = 0.9^{-\frac{1}{100}} = 1.00105416, \color{navy}{\text{very low perplexity}}$$

$$PP\left(W \right) = 10^{-250} => PP\left( W \right) =  \left(10^{-250} \right)^{-\frac{1}{100}} = 316, \color{navy}{\text{very high perplexity}}$$

**Perplexity for bigram model**

$$PP\left(W \right) = \sqrt[m]{\prod_{i=1}^{m} \prod_{j=1}^{ \mid s_i \mid} \frac{1}{ P\left( w_j^{\left(i\right)} \mid  w_{j-1}^{\left(i\right)} \right)   }}, \,\, \color{red}{w_j^{\left(i\right)}\text{: j-th word in i-th sentence}}$$

**If all sentences in the test cases are concatenated:**

$$PP\left(W \right) = \sqrt[m]{\prod_{i=1}^{m}  \frac{1}{ P\left( w_i \mid  w_{i-1} \right)   }} = \sqrt[m]{\prod_{i=1}^{m}  \frac{C\left( w_{i-1}, w_i \right)}{ C \left( w_{i-1}\right)   }} \,\, =\color{red}{w_i \text{: i-th word in test set}}$$


Use log perplexity insetad of perplexity

$$logPP\left(W \right) = -\frac{1}{m}\sum_{i=1}^{m}  log_2\left(P\left( w_i \mid  w_{i-1} \right)  \right), \,\, \color{red}{w_i \text{: i-th word in test set}}$$


#### Out of vocabulary

- closed vocabulary: a fixed last of word.   a set of unique word supported by a language model. e.g. chatbot only answer limited questions
- open vocabulary: can deal with words not seen before (out of vocabulary)
- unknown word also **called out of vocabuarly word (OOV)**
- replace unknown word with special tag ```<UNK>``` in corpus and in input

Using ```<UNK>``` in corpus

1. create vocabulary V
   - Criteria 1: min word frequency f: Ëá≥Â∞ëÂú®corpus ‰∏≠Âá∫Áé∞fÂõû
   - Criteria 2: max size of vocabulary, only include words sort by frequency to the maximum vocabulary size
2. Replace any word in corpus and not in V by ```<UNK>```
3. Count the probabilities with ```<UNK>``` as with any other word, 
    - ```<UNK>``` usually lower perplexity

Note: 

- You might have lot of ```<UNK>```, then model generate a sequence of ```<UNK>``` quotes with high probability instead of meaningful sentences. Due to this limitation, using ```<UNK>``` sparingly
- <span style="background-color: #FFFF00">Only compare Language model perplexity with the same vocabulary</span>

ÊØîÂ¶Ç‰∏ãÈù¢‰æãÂ≠êÔºåvocabulary ÁöÑËØçÂøÖÈ°ªË¶ÅÂú®corpus‰∏≠Ëá≥Â∞ëÂá∫Áé∞‰∏§Âõû, Â¶ÇÊûúÊ≤°ÊúâÂá∫Áé∞‰∏§ÂõûÁî® ```<UNK>``` ‰ª£Êõø

![](/img/post/Natural-Language-Processing/course2/week3pic8.png)



#### Smoothing

Problem: N-gram made of known words still migh be missing in training corpus. ÊØîÂ¶Ç ```eat chocolate. John drinks```, bigram ```John eat``` are missing in bigram

![](/img/post/Natural-Language-Processing/course2/week3pic9.png)


**Add-one smooething (Laplacian smoothing)**: add one occurence to each bigram. bigram that are missing in the corpus have nonzero probability. add one in each cell in the Count Matrix


$$P\left( w_n \mid w_{n-1} \right) = \frac{ C\left(w_{n-1}, w_n\right) + 1}{\sum_{w \in V} C\left( w_{n-1}, W\right) + 1} = \frac{C\left(w_{n-1}, w_n\right) + 1}{C\left(w_{n-1}\right) + V}$$

Note: <span style="background-color:#FFFF00">V is the size of vocabulary (ËÄå‰∏çÊòØtotal count of word)</span>

Plus V only works if real counts are large enough to outweigh the plus one. Otherwise the probability of missing word will be too highÔºåÊØîÂ¶ÇÂ∞±vocubary size = 2, $$ P\left(UNK \mid known word \right) =  \frac{C\left(w_{n-1}, w_n\right) + 1}{C\left(w_{n-1}\right) + V} = \frac{1}{1 + 2} = \frac{1}{3}$$



**Add-k smooething**: make probability even smoother. can be applied to high order n-gram probabilities like trigram, four grams and beyond

$$P\left( w_n \mid w_{n-1} \right) = \frac{ C\left(w_{n-1}, w_n\right) + k}{\sum_{w \in V} C\left( w_{n-1}, W\right) + k} = \frac{C\left(w_{n-1}, w_n\right) + k}{C\left(w_{n-1}\right) + k*V}$$

some other advanced smoothing: Kneser-Ney smoothing, Good-Turing smoothing

e.g. Corpus "I am happy I am learning"  and k = 3, vocabulary size is 4 ```(I, am), (am, happy), (happy, I), (am, learning)```, ```(I, am)``` count is 2

$$P\left(can \mid I\right) = \frac{3}{2 + 3*4}$$


#### Backoff

- <span style="color:red">if N-gram missing => using (N-1) gram. if (N-1) gram missing, using (N-2) gram... so on until find nonzero probabilities</span>
- using lower level N-gram, distort probabilities especially for smaller corpora
  - Probability need to be discounted from high probability to lower probability e.g. *Katz backoff*
  - a large web-scale corpus : *"stupid" backoff*:  no probability discounting applied. If high order n-gram probability is missing, the lower order n-gram probability is used by multiplying a constant. <span style="color:red">0.4 is the constant expertimentally shown to work well</span>


![](/img/post/Natural-Language-Processing/course2/week3pic10.png)


#### Linear Interpolation

linear interpolation of all orders of n-gram: combined weighted probabilities of n-gram, (n-1) gram down to unigrams. <span style="background-color: #FFFF00">Lambda are learned from validation parts of the corpus. Can get them by maximizing the probability of sentences from the validation set</span>. The interpolation can be used for general n-gram by using more Lambdas


$$\hat P\left( w_n \mid w_{n-2}w_{n-1} \right) = \lambda_1 \times P\left( w_n \mid w_{n-2}w_{n-1} \right) +\lambda_2 \times P\left( w_n \mid w_{n-2} \right) + \lambda_3 \times P\left( w_n \right), given \sum_i \lambda_i = 1 $$

e.g. John drink chocolate

$$\hat P\left( chocolate \mid John \, drink \right) = 0.7 \times P\left( chocolate \mid John \, drink  \right) +0.2 \times P\left( chocolate \mid drink \right) + 0.1 \times P\left( chocolate \right) $$

<br/><br/><br/>

## 2.4 Word Embeddings with Neural Networks

Build a vocabulary: assign integer to word, <span style="color:red"> Problem: order doesn't make sense from a semantic perspective. e.g. no reason that happy number > zebra</span> 

![](/img/post/Natural-Language-Processing/course2/week4pic1.png)


#### one-hot vectors

- put 1 in the row that has the same label and 0 everywhere else,
-  <span style="color:red"> Advantage: not imply any relationship between any two words</span>. e.g. Each vector says the word either happy or not , either zebra or not
- <span style="color:red"> Disadvantage:</span>
   -  Huge vectors: <span style="color:red">require a lot of space and processing time</span>. If created with English word, may have 1 million rows, one row for each English word
   -  <span style="color:red">No embedding meaning</span>: if calculate the distance btween one-hot vectors, always get the same distance btween any two pairs of words. ```d(paper, excited) = d(paper, happy) = d(excited, happy)```, (inner product of any 2 one-hot vector is zero). ‰ΩÜÊòØintuitively, happy is more similar to excited than paper


e.g. each vector has 1000 elements,

![](/img/post/Natural-Language-Processing/course2/week4pic2.gif)

#### Word Embedding

ÊØîÂ¶ÇÊ®™ÂùêÊ†áÔºåword are positive right, word are negative left; Á∫µÂùêÊ†á word are concrete, higher. word are abstract lower. <span style="background-color: #FFFF00">Less precise than one hot vector</span>, ÂèØËÉΩÊúâ‰∏§‰∏™ÁÇπÈáçÂêàÔºåÊØîÂ¶Çsnake & spider


![](/img/post/Natural-Language-Processing/course2/week4pic3.png)

‰∏äÈù¢‰æãÂ≠êÊòØ‰∏Ä‰∏™word embedding. 

- Low dimension: Word embedding represent words in  a vector form that has a relative low dimension
- Carry embed meaning:  
  - e.g. semantic distance:  $$\text{forst} \approx \text{tree, forest} \not\approx \text{ticket}$$$
  - e.g. analgies :  Paris -> France similar as Rome -> Italy

![](/img/post/Natural-Language-Processing/course2/week4pic4.png)


**Create Word Embedding**

- If you want to generate word embedding based on Shakespeare, corpus should be full and original text of Shakespeare, not study notes, slide presentation or keywords from Shakespeare. 
- <span style="background-color:#FFFF00">Context(combination of words to occur around that particular word) is important, give meaning to each word embedding</span>. a simple vocabulary list of Shakespeare's most common words not enough to create mebedding. 
- Objective of word embedding is to predict missing word given context words
- Embedding method, machine learning models which are set to learn word embeddings. <span style="color:red">The meaning of words, as carred by word embeddings depends on the embedding approach</span>
  - <span style="background-color:#FFFF00">**Self-supervised learning**: Both unsupervised (input data are unlabeled) and supervised(text provide necessary context which would ordinarily make up the label)</span>
- Word embedding can be tuned by a number of hyperparameters. ÂÖ∂‰∏≠‰∏Ä‰∏™ÊòØ dimension of word embedding vectors. In practice, could from a few hundred to low thousand
  - <span style="color:red">Use high dimension captures more nuanced meanings but computatitional expensive</span>, may leading to diminishing return
- Corpus must first be transformed into a suitable mathematical representation. e.g. Integer word Indices or one-hot vector

![](/img/post/Natural-Language-Processing/course2/week4pic5.png)


- **word2vec**: (Google, 2013). use a shallowed neural network to learn word embedding. propose two model architectures: 
  - Continuous bag-of-words(CBOW): predict a missing word given the surrounding words
  - Continuous skip-gram also know as  Skip-gram with negative sampling (SGNS): did the reverse of Continuous bag-of-words:  model predict surrounding given a input word
- **Global Vectors(GloVe)** (Standford 2014): Involves factorizing the logarithm of the corpuses word co-occurrence matrix.(Similar to Count Matrix as above)
- **fastText** (Facebook 2016): based on skip-gram model and take into account the structure of words by representing words an n-gram of character
  - support out-of-vocabulary(OOV) words: infer embedding from the sequence of chracters they are made of and corresponding sequence that it was intially trained on. E.g. create similar embedding for kitty and kitten even kitty never seen before(Âõ†‰∏∫ kitty and kitten has similar sequences of characters)
  - another benefit: embedding vectors can be averaged together to make vector representations of phrases and sentences
- Advanced work embedding methods: use advanced deep neural network to refine word meanings according  to their contexts. ‰∏ÄËØçÂ§ö‰πâ(polysemy or words with similar meanings), ‰πãÂâçmodel  always has the same embedding e.g. plants. ÂèØ‰ª•ÊòØ organism like a flower, a factory or adverb. below are Tunable pre-trained model available
  - BERT (Google, 2018): Bidirectional representations from transformers by Google
  - ELMo (Allen Institute for AI, 2018)P embeddings from language models
  - GPT-2 (OpenAI,2018): generative pre-training 2


#### Continuous bag-of-words

- <span style="background-color:#FFFF00">the objective: predict a missing word based on the surrounding words</span>
- rationale: if two unique words are both frequently surrounded by a similar set of words in various sentences, then Ëøô‰∏§‰∏™ËØç tend to related in their meaning(related semantically)

![](/img/post/Natural-Language-Processing/course2/week4pic6.png)

- define <span style="background-color:#FFFF00">**context words**</span> as four words. C= 2 <span style="color:red">Two words before center word and two words after it</span>. C is half size of the context. <span style="background-color:#FFFF00">C is the hyperparameter of the model</span>
- Window: center word + context words
- Architecture: context words as input and center words as output

![](/img/post/Natural-Language-Processing/course2/week4pic7.gif)


#### Tokenization

- Case insensitive: The==the==THE  -> convert to either all lowercase or all uppercase
- Punctuation  
  - interruption punctuation marks ```,!.?``` as single special word of the vocabulary.  
  - ignore non interrupting punctuation marks ```'<< >>"```
  - collapse multi-sign marks into single marks ```!!!``` -> ```.``` , ```...``` -> ```.```
- Number: 
  - If numbers do not carry important meaning -> drop
  - number may have meaning. e.g. 3,1415926 -> pi, 90210 is name of television show and zip code => keep as ```/<NUMBER>```
- Special characters: such as mathematical symbols, currency symbols, section and paragraph signs and line markup signs => drop
- Special words: emojis, hastags (like twitter) => depend on if and how handle 
  - can consider each emoji or hashtag considered as individual word. üòÑ  -> ```:happy```


```python
# pip install nltk

#pip install moji

import nltk
from nltk.tokenize import word_tokenize
import emoji

nltk.download('punkt') #download pre-trained Punkt tokenizer for English

# handle common special uses of punctuation

```
‰∏ãÈù¢‰æãÂ≠ê, question marks and exclamation convert to a single full stop, last step get rid of number but keep alphabetical, full stops(previously an interrupting punctuation mark), emoji

![](/img/post/Natural-Language-Processing/course2/week4pic8.gif)


![](/img/post/Natural-Language-Processing/course2/week4pic9.gif)


**Transforming Words Into Vectors**

Corpus: I am happy because I am learning <br/>
Vocabulary: am, because, happy, I, learning  (vocabulary is the set of unique set)


- Center word:  One-hot vector: ‰∏ãÈù¢‰æãÂ≠êorder by alphabetic orderÔºåÂÆûÈôÖ‰∏≠ÂèØ‰ª•ÊòØarbitrary 
- Context word: take the average of one-hot vectors of each context word (average Èô§‰ª•ÁöÑÊòØ 2C, C is half size of context words )

![](/img/post/Natural-Language-Processing/course2/week4pic10.gif)

[Python- Create One hot vector for center /context word ](https://github.com/beckswu/Natural-Language-Processing/blob/master/Coursera/Course2-Natural%20Language%20Processing%20with%20Probabilistic%20Models/week4-data_prep.ipynb)


#### Architecture of Continuous bag-of-words

- based on shallow dense neural network with an input layer, a single hidden layer, and an output layer
- input is vector of context words, size(vector) = size(vocabulary) V
- output is predict of center word, size(vector) = size(vocabulary) V
- Hyparameter: word embedding size N = size (Hidden layer), typical from hundred to thousands
- Need activation function for hidden layer and output layer, result of activation layer is sent to next layer. The outcome of activation of output layer is model prediction
  - <span style="color:red">**Activation function: Hidden layer use ReLu, output layer use softmax**</span>


![](/img/post/Natural-Language-Processing/course2/week4pic11.png)

Dimension(single input):  $$W_1$$ : N (size of word embedding) rows and V (size of vocabulary ) columns

 ![](/img/post/Natural-Language-Processing/course2/week4pic12.gif)


Dimension(Batch input):  <span style="color:red"> m: batch size</span>: hyperparameter model defined at training time

note: $$B_1$$ size is Nx1 broadcasting to NxM


![](/img/post/Natural-Language-Processing/course2/week4pic13.gif)

**Activation Function**

![](/img/post/Natural-Language-Processing/course2/week4pic14.gif)

<span style="background-color:#FFFF00">**Softmax**</span>: input is a vector of real numbers. Output is a vector of real number in the interval 0 to 1 which <span style="background-color:#FFFF00">sum up to 1</span>, ÂèØ‰ª•Ë¢´Ëß£Èáä‰∏∫ probabilities of exclusive events. <span style="background-color:#FFFF00">Each row correspond to a word of the vocabulary: probability of being center word</span>

$$$$$$\hat y_i = \frac{e^{z_i}}{\sum_{j=1}^{V}e^{z_j}}, \text{exponential form transform inputs to all positive }$$

![](/img/post/Natural-Language-Processing/course2/week4pic15.gif)



**Cost Function**

- find the parameters that minimize the loss given the training data sets
- loss function is cross-entropy loss(often used with classification models) -> used in logistic function (log loss)
- larger the loss when prediction incorret(ËßÅ‰∏ãÈù¢‰æãÂ≠ê)
- when $$\hat y$$ close to 1, cross entropy loss close to 0
- loss refer to single example, <span style="background-color:#FFFF00">cost refer to batch input(mean of losses)</span>

$$J_{bactch} = -\frac{1}{m} \sum_{i=1}^m J^{\left(i\right)} = -\frac{1}{m} \sum_{i=1}^m \sum_{j=1}^V y_j^{\left(i\right)} log \hat y_j^{\left(i\right)}$$

![](/img/post/Natural-Language-Processing/course2/week4pic16.gif)

**Forward prop**

![](/img/post/Natural-Language-Processing/course2/week4pic17.gif)

**Backward prop**

- hyperparameter: learning rate $$\alpha$$, alpha is small positive number less than 1.

![](/img/post/Natural-Language-Processing/course2/week4pic18.png)

------------
Mathematical Prove:


As we know:

$$ d\frac{f\left(x\right)}{g\left(x\right)} = \frac{f'\left(x\right) g\left(x\right) - g'\left(x\right)f\left(x\right)}{g\left(x\right)^2}$$

We define simplified notation

$$\text{Denote} \sum_i^V e^{z_i}  \, \text{ as} \sum, \hat y \text{ is the output for softmax function} $$


$$\begin{align} \frac{\partial \frac{e^{z_j}}{\sum} }{\partial z_j} &= \frac{\frac{\partial e^{z_j}}{\partial z_j} \sum - \frac{ \partial \sum }{\partial z_j} e^{z_j} }{ \sum ^2} \\ &=  \frac{ e^{z_j} \sum - e^{z_j}  e^{z_j} }{ \sum ^2} \\ &= \frac{e^{z_j}}{\sum} \frac{\sum - e^{z_j}}{\sum}  = \hat y_j \left( 1-\hat y_j \right) \\
 \frac{\partial \frac{e^{z_j}}{\sum} }{\partial z_k} &= \frac{\frac{\partial e^{z_j}}{\partial z_k} \sum -\frac{ \partial \sum }{\partial z_k} e^{z_j} }{ \sum ^2} \\ &=  \frac{ 0 \sum - e^{z_j}  e^{z_k} }{ \sum ^2}  = -\hat y_k \, \hat y_j  \end{align}  $$

So the derivative of softmax:

$$ \frac{\partial \hat y_i}{\partial z_j} = \begin{cases}
\hat y_j \times \left( 1-\hat y_j \right) ,  & \text{if i = j} \\
-\hat y_k \times \hat y_j , & \text{if i } \neq \text{j}
\end{cases}  $$

$$\text{Note: } y^{\left(i\right)},  \hat y^{\left(i\right)} \text{are label and predict(number) } i^{th} \text{word}$$

$$\text{As we know }\sum_{i=1}^V \hat y_i = 1$$

$$ \begin{align} \frac{\partial Cross-entropy \, Loss}{\partial z_j} &= \frac{\partial  \sum_{i=1}^V y_i log \hat y_i }{\partial z_j} \\
&= \sum_{i=1}^V y_i  \frac{\partial log\hat y_i}{\partial z_j} \\
&= -y_j \frac{1}{\hat y_j} \hat y_j\left(1- \hat y_j\right) - \sum_{i!=j}y_i\frac{1}{\hat y_i}\left(-\hat y_i \hat y_j \right) \\
&= -y_j + \color{blue}{y_j}\hat y_j + \hat y_j \color{blue}{\left(\sum_{i!=j}^V y_i \right)}\\
&= -y_j + \hat y_j \left( \sum_{i}^V y_i \right) =  \hat y_j-y_j
\end{align} $$

Therefore: Note $$H_j$$ is the hidden layer output
 
$$\frac{\partial Cross-entropy \, Loss}{\partial w_j} = \frac{\partial Cross-entropy \, Loss}{\partial z_j} \frac{\partial z_j }{\partial w_j} = \left(\hat y_j-y_j \right) H_j  $$



------------


#### Extract Word Embedding Vectors

After iterate training data serval times,

**Option 1**:  can consider <span style="color:red">each column of $$W_1$$ as embedding vector of a word of the vocabulary</span>, $$W_1$$ size is VxN, it has one column for each word in the vocabulary, ÊØîÂ¶Ç‰∏ãÈù¢‰æãÂ≠êÔºåÊÄªÂÖ±5ÂàóÔºåÁ¨¨‰∏ÄÂàó is the word embedding column vector for *am*, Á¨¨‰∫åÂàófor *because*

![](/img/post/Natural-Language-Processing/course2/week4pic19.png)


**Option 2**: Use <span style="color:red">each row in $$W_2$$ as the embedding row vector for corresponding word</span>. $$W_1$$ size is NxV,  order is the same as input vector or matrix ÊØîÂ¶Ç Á¨¨‰∏ÄË°åË°®Á§∫ embedding row vector for *am*, Á¨¨‰∫åË°åfor *because*

![](/img/post/Natural-Language-Processing/course2/week4pic20.png)


**Option 3**: average of Option 1 and Option 2, average $$W_1$$ and the transpose of $$W_2$$  , $$W_3$$ size is NxV, <span style="color:red">each column of $$W_3$$ as embedding vector of a word of the vocabulary</span>, e.g. Á¨¨‰∏ÄÂàó is the word embedding column vector for *am*, Á¨¨‰∫åÂàófor *because*

![](/img/post/Natural-Language-Processing/course2/week4pic21.png)



#### Evaluation


- Intrinsic evaluation: assess how well word embedding inherently capture the semantic(meaning of words) or syntactic(grammar) relationship between words
  - could testing word embeddings on analogies:  
      - Semantic analogies: e.g. France -> pairs as Italy -> ?
      - syntactic analogies: e.g. plural, tenses(Êó∂ÊÄÅ), and comparatives(ÊØîËæÉÁ∫ß) e.g. seen -> saw as been -> ?
      - <span style="color:red">should aviod ambiguiy: may several correct answer</span> e.g. wolf -> pack as bee -> swarm? colony?
  - Clustering: <span style="background-color:#FFFF00">group similar word embedding vectors</span>, capture related words. could be auomated using human made reference such as thesaurus(ÂàÜÁ±ªËØçÊ±áÊ±áÁºñÔºõÁôæÁßëÂÖ®‰π¶)
      - ÊØîÂ¶Ç‰∏ãÈù¢‰æãÂ≠ê hair care & hairdressing close to each other
- Extrinsic evaluation:use word embeddings to perform an external task, typically real world task that initially needed the word embeddings for. Then use performance metric of this task as a proxy for the quality of word embeddings
  - task include: named entity recognition, parts-of-speech tagging 
      - A named entity can be referred to a proper name. e.g. "Andrew works at deeinglearning.ai." Andrew is an entity and categorized as a person. deeinglearning.ai is entity categorized organization
  - evaluate classifier on the test set e.g. accuracy, F1-score. <span style="background-color:#FFFF00">The classifier's performance on evaluation metric represents combined performance of both the embedding and classification tasks.</span>
  - Extrinsic evaluation are the ultimate test that word embedding are useful
  - **Drawbacks**: 
    - <span style="color:red"> time-consuming than intrinsic evaluation.</span>
    -  <span style="color:red"> More diffcult to trouble shoot. If performance is poor, performance metrics provide no information Âì™ÈáåÂá∫Èîô‰∫Ü because word embedding are external test used to test them</span>

‰∏ãÈù¢Intrinsic evaluation axample are from original Word2vec research publication with word embeddings created from huge training set. Created by continuous skip-gram model not continuous bag of words model. But evaluation principle is the same. Note: analogies are not perfect. ÊØîÂ¶Ç uranium ÁöÑÂåñÂ≠¶Á¨¶Âè∑ÊòØ U, but word2vec outputs plutonium

![](/img/post/Natural-Language-Processing/course2/week4pic22.gif)

#### Python Lib

Keras and Pytorch add embedding layer in neural network model using a sinle line of code. Other library like Trax also allow you do so.

```python
#from keras.Layers.embeddings import Embedding

embed_layer = Embedding(10000,400)

#import torch.nn as nn

embed_layer = nn.Embedding(10000,400)
```


## 3.1 Trax: Neural Networks


Neural Network predict positive/negative tweets, structure:

- has a embedding layer that transform the representation 
- has another hidden layer with a ReLU activation function 
- output layer with softmax function that give probabilities for whether a tweet is positive or negative sentiment
- This structure allow to predict complex tweets: ÊØîÂ¶Ç "This moive was almost good". ‰∏çËÉΩ classify correctly using simpler method ÊØîÂ¶Ç Naive Bayes
- Initial Representation for neural network is a vector of Integers, <span style="background-color:#FFFF00">List all word from vocabulary then assign integer index to each of word. Then need to Áü•ÈÅì maximum vector size then fill zeros to match that size (called **padding**: ensures that all vector have the same size even if your tweets don't)</span>

![](/img/post/Natural-Language-Processing/course3/week1pic1.png)

![](/img/post/Natural-Language-Processing/course3/week1pic2.png)

#### Trax

List the layer from left to right. Note Dense layer(fully-connected layer): A maps collections of R^m vectors to R^n, where n (= n_units) is fixed at layer creation time,. Fully computation is ```y = Wx + b```. Generally followed by a non-linear activation function. 

![](/img/post/Natural-Language-Processing/course3/week1pic3.png)



Advantage: (Trax is based on Tensorflow, use Tensorflow as a backend, also use **JAX** library to speed up computation)

- Run fast on CPUs, GPUs and TPUs(perform computations efficiently), don't need to change a single line of code for running on CPU, GPU, TPU
- allow parallel computing: allow model running on multiple machines or courses simultaneously
- Record algebraic computations for gradient evaluation, so they compute gradient automatically. 


Trax Development History: TensorFlow (2014-2015) -> Translate(2016, long time to train, not practical for anyone else other than Google) -> Tensor2Tensor(2017, userd by many large companies in the word. Complicated but not nice to learn, hard for new researchers) -> Trax

TraxÔºö Deep Learning library with Clear Code and Speed

What do you want from a deep learning library? 

- Makes programmers efficient: Easy to Debug and understand 
- Run code fast: Trax can run on-demand or preemptible Cloud without changing a single line of code 


Some Resource

- [Official Trax documentation maintained by the Google Brain team](https://trax-ml.readthedocs.io/en/latest/)
- [Trax source code on GitHub](https://github.com/google/trax)
- [JAX library](https://jax.readthedocs.io/en/latest/index.html)

Python Class & Subclass(subclass can override method in super class )

![](/img/post/Natural-Language-Processing/course3/week1pic4.gif)

#### Trax Layers

**Dense Layer**: The single computation( compute inner product between trainable wights and input vector) is called Dense layer

![](/img/post/Natural-Language-Processing/course3/week1pic5.png)

**ReLU Layer**: activation layer that follows a dense fully connected layer, and transforms any negative values to zero.

![](/img/post/Natural-Language-Processing/course3/week1pic6.png)

**Serial Layer**: a composition of sublayers layers that operates sequentially to perform the forward computation of entire model. Can think of serial layer as whole neural network model in one layer

![](/img/post/Natural-Language-Processing/course3/week1pic7.gif)

**Embedding Layer**: map words from vocabulary to representation of that word with determined dimension, ‰∏ãÈù¢‰æãÂ≠ê embedding size = 2,

![](/img/post/Natural-Language-Processing/course3/week1pic8.png)

**Mean Layer**: Take means of each features from embedding. After mean leary, will have the same number of featurres as embedding size. Note: <span style="color:red">Ëøô‰∏™layer Ê≤°Êúâ‰ªª‰Ωïtrainable parameters because it's only computing the mean of word embeddings</span>
    - ```trax.layers.tl.Mean```: Calculates means across an axis. In this case, please choose axis = 1 to get an average embedding vector (an embedding vector that is an average of all words in the vocabulary).
    - For example, if the embedding matrix is 300 elements and vocab size is 10,000 words, taking the mean of the embedding matrix along axis=1 will yield a vector of 300 elements.

```python
# Pretend the embedding matrix uses  2 elements for embedding the meaning of a word

# and has a vocabulary size of 3, So it has shape (2,3)

tmp_embed = np.array([[1,2,3,],
                    [4,5,6]
                   ])
print("The mean along axis 0 creates a vector whose length equals the vocabulary size")
display(np.mean(tmp_embed,axis=0)) # DeviceArray([2.5, 3.5, 4.5], dtype=float32)

print("The mean along axis 1 creates a vector whose length equals the number of elements in a word embedding")
display(np.mean(tmp_embed,axis=1)) # DeviceArray([2., 5.], dtype=float32)

```

![](/img/post/Natural-Language-Processing/course3/week1pic9.png)

#### Gradient in Trax

- function ```grad``` return a function that computes the gradients of f
  - allows much easier training
- forward and backward in a single line 
  - Notice the double parentheses, Á¨¨‰∏Ä‰∏™ÊòØ passes the arguments for the grad function, Á¨¨‰∫å‰∏™ÊòØarguments for function returned by grad

![](/img/post/Natural-Language-Processing/course3/week1pic10.png)

![](/img/post/Natural-Language-Processing/course3/week1pic11.png)


## 4.2 RNN for Language Modeling

**Problem with N-grams**: for N grams, model have to account for conditional probabilites for long sequences, difficult to estimate for long corpora: Need a lot of space and RAM. e.g. if user download app, may not have enough space. 

![](/img/post/Natural-Language-Processing/course3/week2pic1.gif)

#### RNN 

**RNN**: Looks at previous N words, propagate the information from the beginning to the end

e.g. ‰∏ãÈù¢‰æãÂ≠êÔºåtrigram probabable choose "have": because highly probable to see that sequence of three words in text corpus. ‰ΩÜÊòØÂ¶ÇÊûúÁî® have, sentence not make sense. Â¶ÇÊûúwant to have a n-gram, have to account 6-grams, impractical

![](/img/post/Natural-Language-Processing/course3/week2pic2.gif)

Each box represent the computation made at each steps and colors represent the information used for every computation/ Every word in the sequence is multiplied by the same weight, $$W_x$$, the informatin propagate from the beginning to the end is multiplied by $$W_h$$. The block is repeated for every word in the sequence. The only learning parameters are $$W_x, W_h$$. They compute values that are fed over and over again until the prediction is made.

Note:An RNN would have the same number of parameters for word sequences of different lengths. (Computations share parameters)

![](/img/post/Natural-Language-Processing/course3/week2pic3.gif)


Many to Many architecture, at each time step, <span style="background-color:#FFFF00">two inputs, x, hidden state h</span>. Make a prediction $$\hat y$$. Additionally, it propagate a new hidden state to the next time-step. The hidden state at every time t is computed with an activation function g

 $$ h^{<{t}>} = g\left(W_h \left[  h^{<{t-1}>},  x^{<{t}>} \right] + b_h \right)$$

 $$ h^{<{t}>} = g\left(W_{hh} h^{<{t-1}>} \oplus  W_{hx} x^{<{t}>} + b_h \right) \quad  \oplus \text{sum up element-wise}$$

$$ W_h = \left[W_{hh} \mid W_{hx} \right] \quad \color{red}{\text{Horiztonal Stack}}$$

$$\left[  h^{<{t-1}>},  x^{<{t}>} \right]= \left[ \frac{h^{<{t-1}>}}{x^{<{t}>}} \right] $$



- ùëä‚Ñé  in the first formula is denotes the horizontal concatenation of ùëä‚Ñé‚Ñé and ùëä‚Ñéùë• from the second formula.

- ùëä‚Ñé in the first formula is then multiplied by  $$\left[  h^{<{t-1}>},  x^{<{t}>} \right] $$, another concatenation of parameters from the second formula but this time in a different direction, i.e vertical!

![](/img/post/Natural-Language-Processing/course3/week2pic5.png)

First cell for RNN, take inputs the previous hidden state and current variable x, to get the current hidden state, get the product of x and $$h^{<{0}>$$ with their respective parameters, then sum the vectors element-wise. Then pass to activation function. With resulting value, compute current $$\hat h$$ by going through another activation function

![](/img/post/Natural-Language-Processing/course3/week2pic6.gif)

e.g. if $$ h^{<{t}>}$$ size is 4 x 1, $$ x^{<{t}>}$$ size is 10 x 1, what is the size of $$W_h$$, given  $$ h^{<{t}>} = g\left(W_h \left[  h^{<{t-1}>},  x^{<{t}>} \right] + b_h \right)$$

Q: 14 x 4



#### Application

- One to One:  ÊØîÂ¶ÇÁªôÂá∫lists of score from favorite team to predict the position of your team on the leaderboard
- One to Many: ÁªôÂõæÁâá predict caption
- Many to One: Sentiment Analysis: RNN take every word from the sequence as inputs, propagate information from the beginning to the end, and output the sentiment
- Many to Many: machine translation: encode and decoder architecture is popular for machine translation. First half doesn't return output is called encode, because it encodes the sequences of words in a single representation that caputres the overall meaning of the sentence. Decoder generate a sequence of words in other language.


![](/img/post/Natural-Language-Processing/course3/week2pic4.gif)


#### Cost Function


k: the number of category/class, $$y_j = 0 or 1$$ ÂØπ‰∫éÊØè‰∏™class, ÊØîÂ¶Ç‰∏ãÂõæoutput ÊòØ3‰∏™class

![](/img/post/Natural-Language-Processing/course3/week2pic7.png)

For RNN: 

$$ J = -\frac{1}{T} \sum_{t=1}^T \sum_{j=1}^K y_j^{<{t}>} log \hat y_j^{<{t}>} $$, ‰∏é‰∏äÈù¢ÊôÆÈÄöneural network cost function difference is summation over time T and division by the total number of steps T: is the <span style="background-color:#FFFF00">average of the cross entropy loss function over time</span>

![](/img/post/Natural-Language-Processing/course3/week2pic8.png)

#### Scan Function

- Scan function is to take a function fn( equivalent to ‰∏ãÂõæ‰∏≠ÁöÑ $$f_w$$), and apply to all the elements from beginning to end in the list ```elems``` ($$x{<{t_1}>}, x{<{t_2}>} ... x{<{T}>}$$))
- Initializer (the hidden state $$h^{<{t_0}>}$$) is an optimal variable that could be used in the first computation of fn
- Function first initialize hidden states $$h^{<{t_0}>}$$, set ys as an empty list  
- For every x in the lis of elements, fn is called with x and the last hidden state as arguments, 
- For loop computes every time step for RNN, stores prediction value and hidden states
- Finally, function <span style="color:red">returns a list of predictions, and last hidden state</span>
- <span style="color:red">Framework like Tensorflow need this type of abstraction in order to perform parallel computions and run on GPUs.</span>

![](/img/post/Natural-Language-Processing/course3/week2pic9.gif)

## Gated Recurrent Units

- control how much information forget from the past and how much information to extract from current input. 
- allow relevant information to be kept in the hidden states, even over long sequences. ÊØîÂ¶Ç‰∏ãÈù¢‰æãÂ≠êÔºåAnts learn Plural. GRU complete it by computing irrelevance and update gates. Can GRUS as Vanilla RNN with additional computations

$$\Gamma_r = \sigma \left(W_r \left[  h^{<{t-1}>},  x^{<{t}>} \right] + b_r \right)  $$

$$\Gamma_u = \sigma \left(W_u \left[  h^{<{t-1}>},  x^{<{t}>} \right] + b_u \right)  $$

$$h^{'<{t-1}>} = tanh\left(W_h \left[ \Gamma_r *  h^{<{t-1}>},  x^{<{t}>} \right] + b_h \right)  $$

$$h^{'<{t}>} = \Gamma_u *  h^{<{t-1}>} + \left( 1- \Gamma_u\right) * h^{'<{t-1}>} $$

$$y^{<{t}>} = g\left(W_y  h^{<{t}>} + b_y \right)$$


1. The first two computation in GRU are <span style="background-color:#FFFF00">**relevance gates**</span>. $$\Gamma_r, \Gamma_u $$ computed from sigmoid function. <span style="color:red">The output help determine which information from the previous hidden state is relevant and which values should be updated with current informations</span>
2. After relevante gates is computed, the candidate h' for the hidden state is found: the value store all the candiates for information that overwrites in previous hidden states.  
3. A new value for the current hidden state is calculated using the information from the previous hidden state, candidate hidden state and update gate and update some information from the last hidden states. <span style="background-color:#FFFF00">Updte date determnnes how much of information from the previous hidden state will be overwrited</span>. 
4. a prediction $$\hat y$$ is computed using current hidden state

![](/img/post/Natural-Language-Processing/course3/week2pic10.png)

Vanilla RNN vs GRUS: 

- **Problem with Vanilla**: update the hidden state at every time step, for a long sequences, information tends to vanish. So called <span style="background-color:#FFFF00">**vanishing gradients problem**</span>(ÂºÄÂßãÁöÑ‰ø°ÊÅØÂèØËÉΩ‰∏¢Â§±‰∫Ü)
- GRUs compute significantly more operations, can cuase lnger processing times and memory usage. GRUS help preserve important information for longer sequences. 

![](/img/post/Natural-Language-Processing/course3/week2pic11.gif)



#### Deep Bidiretion RNN

e.g. ÊØîÂ¶Ç‰∏ãÈù¢‰æãÂ≠êblank Êù•Ëá™‰∫éÂêé‰∏ãÊñá

![](/img/post/Natural-Language-Processing/course3/week2pic12.png)

Could have another architecture where the information flow from end to beginning. 

- <span style="background-color:#FFFF00">Information flows from the past(from left to right) and from the future(from right to left) independently</span>
- Get prediction $$\hat y$$, <span style="color:red">have to start propagate information from both directions</span>
- After compution all the hidden states for both directions, can get all of remaining predictions. 

$$y^{<{t}>} = g\left(W_y  \left[ \vec h^{<{t}>}, \overleftarrow h^{<{t}>} \right]  + b_y \right)$$

![](/img/post/Natural-Language-Processing/course3/week2pic13.gif)

Deep RNNs are similar to deep neural networks. Deep RNN have a layer which takes input sequence x and multiple additional hidden layers. Like RNN stack together

$$h^{\left[l \right]<{t}>} = f^{\left[l \right]} \left(W_h^{\left[l \right]}  \left[ h^{\left[l \right]<{t-1}>} , a^{\left[l-1 \right]<{t}>}  \right]  + b_h^{\left[l \right]}  \right)$$

$$a^{\left[l \right]<{t}>} = f^{\left[l \right]} \left(W_a^{\left[l \right]} h^{\left[l \right]<{t}>}   + b_a^{\left[l \right]}  \right)$$

1. Get hidden states for current layer: Af first progragate information through time. 
2. pass the activations to next layer. Go deeper in the network and repeat the process for each layer until get prediction
  

![](/img/post/Natural-Language-Processing/course3/week2pic14.gif)


## 3.3 LSTMs and Named Entity Recognition

**RNN Advantages**:

- Capture dependecies within a short range
- Takes up less RAM and space(lightweight) than other n-gram models

**RNN Disadvantages**:

- Struggles with longer sequence
- Prone to vanishing or exploding gradients: 
  - This can arise due to the fact that RNN progagates information from the beginning of the sequence to the end. Starting with first words, the first values are computed(‰∏ãÂõæÊ©ôËâ≤Ê°Ü). Then it propagates some computed information takes the second word in the sequence, an gets new value(‰∏ãÂõæÁªøËâ≤Ê°Ü). Final steps, computattions contain information from all the words in the sequence, then RNN predict next word. <span style="color:red">Notice: the information from first step doesn't have much influence on the output.</span>. Can see Orange portion from the first step decrease with each new step. Âõ†‰∏∫computation at the first steps don't have much influence on the cost function. 

![](/img/post/Natural-Language-Processing/course3/week3pic1.png)

#### Vanishing/Exploding Gradient 

Backprop:  The derivatives from each layer are multiplied from back to front in order to compute the derivative of initial layer.  The weights receive an update is proportional to the gradients with respect to the current weights of that time step. But in the network with<span style="color:red"> many time steps or layers, gradient arrived back at early layers as the product of all the terms from the later layer -> make unstable situation. Especially if the values become so small -> no longer updates properly</span>. For exploding gradients, work in opposite direction <span style="color:red">as updated weights become so large causing network to become unstable.</span>(numerical overflow)

![](/img/post/Natural-Language-Processing/course3/week3pic2.png)

Solution: 

- **Identity RNN with ReLU activation**: Initialize weights as indentity matrix: what it does, copy previous hidden state, add information from current inputs and replace any negative values with 0. Has effects of your network to stay close to the values in the indentity matrix which act like 1s during matrix multiplication
  - <span style="background-color:#FFFF00">**only works for vanishing gradients**</span>, as derivates of ReLu = 1 for all values greater than 0
- **Gradient clippings**: simply choose a relevant value that clip the gradients e.g. 25. Using this technique, any value grater than 25 will be clipped to 25
  - <span style="background-color:#FFFF00">**works for exploding gradients**</span>. serves to limit the magnitute of gradients. 
- **Skip connections**: provide a direct connection to earlier layers, effectively skips over activation functions, add values from initial inputs x to outputs, e.g. F(x) + x. so <span style="background-color:#FFFF00">activations from eariler layers have more influence over the cost function</span>


![](/img/post/Natural-Language-Processing/course3/week3pic3.png)

‰∏ãÂõæÂèØËÉΩÊúâvanishing gradient problem, because for a good model, performance should improve accuracy through time

![](/img/post/Natural-Language-Processing/course3/week3pic4.png)



#### LSTM

**Application for LSTM**

- Next-character prediction for email
- chatbots capable of remembering longer conversations
- musice composition: consider music is built using long sequences of notes, much like text uses long sequences of words
- image caption
- speech recognition

LSTM: special variety of RNN handle entire sequences of data by learning when to remember and when to forget, similar to GRU

- A cell state: can think of as memory
- A hidden state with three gates(<span style="color:red">flow È°∫Â∫èÊòØforget gate, input gate, output gate</span>): computation perform during training to decide what changes to make. has three gates to pass though before the entire operation is performed again. Any loop would update the weights. These cell state travels through these three gates that track inputs as it arrives,.Each one plays a part deciding how much information to pass and how much to leave behind. The series of gates allows gradients to flow unchanged. -> so vanishing/exploding gradient is mitigated
  - Forget gate decides what to keep
  - Input gate decides what ot add
  - Output gate decides what the next hidden state will be
- Loop back again at the end of each time step. 

Structure:



1. cell state $$c^{<{t_0}>}$$: can think of cell as memory of your network, carrying all the relevant information down the sequence. 
  - As cell travels, each gate adds or removes information from cell state
2. ÊúÄÂ∑¶ËæπÊòØforget gate, typically sigmoid layer, looks at all the information from previous cell state and decides what to throw away. Sequeeze each value from the cell states between zero and one
  - <span style="background-color:#FFFF00">If value close to zero -> indicates it should be thrown away. Value close to one -> indicates should be kept</span>
  - take the value from the forget states and multiply them element-wise by cell states
3. Input gate: decide what new information to store/update in the cell, has two parts: 
  1. sigmoid layer take the previous hidden state  $$h^{<{t_0}>}$$ and current input  to choose which values to update by assigning zero or one to each value. The closer to one, the higher the importance
  2. tanh layer to slect candidates for new values to be added. take the previous hidden state  $$h^{<{t_0}>}$$ and current input to sequeezes the values between -1 and 1
  3. Then the output of sigmoid and tanh are multiplied, model has what to need to calculate new cell state
4. Then calculate new cell state.  $$c^{<{t_1}>}$$
5. Finally output gate ($$y^{<{t}>}$$) takes the previus hidden state along with the current inputs and process them through another sigmoid and tanh layer. The outputs from sigmoid layer are multiplied by te tanh layer's output. The product decides how much write to next hidden state. 

![](/img/post/Natural-Language-Processing/course3/week3pic5.gif)


Note: The tanh layer‚Äôs output (between -1 and 1) is multiplied element-wise by the sigmoid layer‚Äôs output in both the input and output gates, ensuring an even flow through the LSTM unit.  This prevents any of the values from the current inputs from becoming so large that they make the other values insignificant. Ensures the values in your network stay numerically stable,


#### NER

NER: Named Entity Recognition: fast and efficient way to scan text for certain kinds of information. NERs sytems locate and extract named entities from text. Name entities can be a place, an organization or a person's name, can even be tiems and dates.  


![](/img/post/Natural-Language-Processing/course3/week3pic6.gif)


- B dashes denotes entity 
- All other words are classified O

![](/img/post/Natural-Language-Processing/course3/week3pic7.png)

Application:

- Search engine efficiency: NER models scan millions of websites once and stores the entities as indentified in the process. Then tag from your search query simply match against website tags. 
- recommendation engines: tags extracted from your history and compared to similar user hitories then recommend things you might want to check out
- Customer service: match customer to an appropriate customer service agents. works similarly to a phone tree where prompted to provide some information about your request. e.g. use an app to communicate with your car insurance company, ÈúÄË¶Åprovide some spoken information. then match to appropriate agents
- Automatic trading: build a web scraper to get the articles of the names of certain CEOs, Ssocks, or cryptocurrencies. Then feed those article into a sentiment classification system and make your trade accordingly

**Training NERS: Data Processing**

1. Assign each entity class to a unique number. e.g. person name = 1, geographical location = 2, time indicator = 3
2. Assign each word a number that corresponds to its assigned entity class. O Ë°®Á§∫ a failure or unrecognized word. <span style="color:red">Numbers are random and assigned when you process your data</span>. Each sequence is transformed into an array of numbers where each number Ë°®Á§∫ index of the labeled word
3. apdding: LSTM require all sequences are the same length. Can set length of sequnces to a certain number and add generic ```<PAD>``` token to fill all empty spaces.


![](/img/post/Natural-Language-Processing/course3/week3pic8.png)

**Traing the NER**:

1. Create a tensor for each input and corresponding number 
2. Put them in a batch, batch size normally power of 2, <span style="color:red">speed up processing time considerably</span> -> 64, 128, 256, 512...
3. Feed it into an LSTM unit
4. Run the output through a dense layer (Linear operation on the input vectors) or fully connected layer
5. Predict using a <span style="background-color:#FFFF00">**log softmax**</span> over K classes (K: the number of possible output), <span style="color:red">log softmax gets better numerical performance and gradients optimization than softmax</span>

![](/img/post/Natural-Language-Processing/course3/week3pic9.png)


**Evaluateig the model**

1. Pass test set through the model
2. Get arg max across the prediction array, ‰∏ãÈù¢```np.argmax``` takes an axis parameter. need to consider the dimension of array
3. make sure arrays are padded using ```<PAD>``` token, which makes arrays the same length
4. Compare outputs against test labels to see how accurate model is


‰∏ãÈù¢code, mask variable is where you identify any token IDs you need to skip over during evaluation. One token need to skip is ```<PAD>``` token

![](/img/post/Natural-Language-Processing/course3/week3pic10.png)

**Some Useful Link**



[**Intro to optimization in deep learning: Gradient Descent**](https://blog.paperspace.com/intro-to-optimization-in-deep-learning-gradient-descent/)

[**Understanding LSTMs**](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)

## 3.4 Siamese Network

It's made up two identical neural networks which are merged at the end.  <span style="background-color:#FFFF00">Used as Indenify similarity between things</span>

e.g. Comparing meaning not the same as compare words

- How old are you = what is your age
- Where are you from $$/neq$$ where are you going Â∞ΩÁÆ°Ââç‰∏â‰∏™Â≠ó‰∏ÄÊ†∑Ôºå‰ΩÜÊòØ‰∏çÁ≠â

Application:

- Question duplicates: Stackoverflow, Quera check your questions if exist before post
- Handwritten checks: determine if two signatures are the same or not
- Search engine queries: to predict whether a new query is similar to the one that was already executed

#### Architecture

- <span style="background-color:#FFFF00">Both network have a identical sub network.</span>  <span style="color:red">Âπ∂‰∏îsub networks share identical parameters</span>. Learned parameters of each sub-network are exactly the same. 
- Each network output a vector, one for each questions.
- Cosine similarity tells how similar they are, $$\hat y$$ a value between -1 and 1
  - <span style="background-color:#FFFF00">if $$\hat y \leq \tau $$ -> input questions are different. if $$\hat y > \tau $$ -> input questions are the same</span>. $$\tau$$ is a parameter that chose how often want to interpret cosine similarity to indicate two questions are similar. Higher threshold means only very similar sentences is considered similar


‰∏ãÈù¢ÊòØ‰∏Ä‰∏™example, not all Siamese network contain LSTM. 

![](/img/post/Natural-Language-Processing/course3/week4pic1.png)


#### Cost Function

- Anchor: the first question e.g. "How old are you"
- positive questions: have the same meaning as anchor, cosine similarity close to 1
- negative questions: not have the same meaning as anchor, cosine similarity close to -1

![](/img/post/Natural-Language-Processing/course3/week4pic2.png)

$$ diff = s\left(A,N \right) - \left(A, P \right) = \frac{A \cdot N}{\Vert A \Vert \Vert N \Vert}- \frac{A \cdot P}{\Vert A \Vert \Vert P \Vert}$$

<span style="color:red">s(v1, v2) is similarity metric(cosine similarity), could be distance metrics d(v1, v2) such as ecludian distance </span>

loss function allow whether model do well. ÂΩìdifference igger or smaller along x-axis, loss bigger or smaller along the y-axis. Minimizing loss in training-> minimize this difference


![](/img/post/Natural-Language-Processing/course3/week4pic3.png)

Notice, when difference less than zero, want loss < 0? give model positive value, model update and improve. give model negative value, like telling: Good job. Please update weight to the worst next time. When loss = 0, not asking model to update weights, because performing as expected for that training example. Need to update loss function as below. 

Â¶ÇÊûúdifference is just less than zero, want model to still learn and <span style="background-color:#FFFF00">ask to predict a wider difference, can shift the loss function a little to the left. margin refer as *Alpha*.</span> Loss function shift to right by $$\alpha$$. So if difference less than 0 but small in magnitute, loss greater than 0.

![](/img/post/Natural-Language-Processing/course3/week4pic4.png)


**Triplet Loss**

$$L = \begin{cases}0; \quad \text{if diff } + \alpha \leq 0 \\[2ex] diff; \, \text{if diff } + \alpha > 0 \\ \end{cases} $$

$$L\left(A, P, N \right) = max\left(diff + \alpha, 0 \right) $$




#### Triplet Selection

  
- Firstly, select pair of duplicate questions as anchor and positive 
- Secondly, select a question is difference meaning from the anchor, to form anchor and negative pair
- ÈîôËØØÁöÑÊñπÂºèÊòØ: select triplet random: <span style="color:red">ÂæàÂèØËÉΩselect non-duplicative pair A and N, where loss 0. Loss is zero whenever model correctly predict A and P more similar to A and N. When loss is zero, network little to learn from triplet.</span>
- **Hard Triplet**: <span style="background-color:#FFFF00">are more difficult to train. Hard triplets ÊòØsimilarity between anchor and negative very close to, but still smaller than similarity between anchor and positive.</span> When model encounter hard triplet, algorithm need to adjust its weight so that yield similairty line up with labels
  - By selecting hard triplets, trainning focus are doing better on difficult caes which ‰πüËÆ∏‰ºöpredict incorrectly

![](/img/post/Natural-Language-Processing/course3/week4pic5.png)


Use ```b``` as batch size, Â¶Ç‰∏ãÂõæ<span style="color:red">ÊØè‰∏ÄË°åÔºåÊòØduplicate, ÊØè‰∏Ä‰∏™column Ê≤°Êúâduplicate</span>

![](/img/post/Natural-Language-Processing/course3/week4pic6.png)

vector v1, #row = batch sizes,  #columns is the number of embedding layer(```d_model```). Note dimension of embedding ```d_model``` is a parameter that determins the dimensions of weights through each layer in the model, determine the size of output layer. $$v_{1,1}$$ is a duplicateo of $$v_{2,1}$$, but $$v_{1,1,}$$ is not a duplicates of any other row in $$v_{1}$$

![](/img/post/Natural-Language-Processing/course3/week4pic7.png)

may get similarity as ‰∏ãÂõæÂ∑¶‰æßÁöÑmatrix

- Digonal is the key features, <span style="background-color:#FFFF00">These values are the similarities for positive examples, question duplicates. ÈÄöÂ∏∏diagonal Êï∞ÊØî‰∏çÊòØÂú®diagonalÁöÑÊï∞Ë¶ÅÂ§ß, because expect duplicates have higher similarity than non-duplicates. </span>
- upper right, lower left -> similarity for all negative examples. most number are lower than similarities along the diagonal
- Ê≥®ÊÑè negative example question pairs still have similarity Â§ß‰∫é0, ËôΩÁÑ∂similarity range ‰ªé-1 Âà∞1Ôºå‰ΩÜ‰∏çÊÑèÂë≥ÁùÄÂ§ß‰∫é0 Ë°®Á§∫duplicates ÊàñÂ∞è‰∫éË°®Á§∫non-duplicated. <span style="background-color:#FFFF00">Really mater ÊòØfind duplicates have a higher similarity relative to non-duplicated.</span>
- <span style="color:red">Creating pairs like this remove the need for additional  non-duplicate example and input data</span>. Insteading of sets up specific bathchs with negative examples, model can learn from existing questions duplicates batches.

Overall Cost Function 

$$\begin{align}J &= \sum_{i=1}^{m} L\left(A^{\left(i\right)}, P^{\left(i\right)}, N^{\left(i\right)} \right) =\sum_{i=1}^{m}  max\left(diff + \alpha, 0 \right) \\ &= \sum_{i=1}^{m}  max\left(s\left(A^{\left(i\right)}, N^{\left(i\right)} \right) - s\left(A^{\left(i\right)}, P^{\left(i\right)}\right) + \alpha, 0 \right) \\&  \color{blue}{\text{i refer to a specific training example and there are m observations}}\end{align}$$

![](/img/post/Natural-Language-Processing/course3/week4pic8.png)

- **Mean negative**: mean of off-diagonal values in each row(<span style="color:red">mean of the similarity of negative examples, not mean of negative number in a row</span>). ÊØîÂ¶ÇÁ¨¨‰∏ÄË°å (-0.8 + 0.3 - 0.5)/3
  **Closest negative**: off diagonal value cloest to(but less than) the value on diagonal in each row. By choosing this, force model to learn what diffferentiates these examples and drive those similarity values further apart through training. ÊØîÂ¶ÇÁ¨¨‰∏ÄË°åÈÄâÂèñ0.3: meaning a similarity of 0.3 has the most to offer model in terms of learning opportunity

$$L_{Original} = max \left(\underbrace{s\left(A,N \right)-s\left(A,P \right)}_{diff} + \alpha, 0 \right)$$

$$L_{1} = max \left(mean\_neg+ \alpha, 0 \right)$$

$$L_{2} = max \left(closest\_neg+ \alpha, 0 \right)$$

$$L_{Full}\left(A, P, N \right) = L_{1} + L_{2} $$

$$J = \sum_{i=1}^m L_{Full}\left(A^{\left( i \right)}, P^{\left( i \right)}, N^{\left( i \right)} \right) $$


In order to minimize the loss, you want $$diff + \alpha \leq 0$$, 

- **Loss 1**: <span style="background-color:#FFFF00">using mean negative replace similarity of A and N, help model converge faster during training by reducing noice</span>. Reduce noice by trainning on average of several observations rather than training model on each of these off-diagonal examples. 
  - Why Reduce noise? We define noise to be a small value that from a distribution that is centered arund 0. So the average of several noise is centered around 0.(Cancel oout individual noise from those observations)
- **Loss 2**: create a slightly large penalty by diminishing the effects of more negative similarity of A and N. <span style="color:red">can think of closest negative as finding negative example that results in smallest difference between two similarity</span>. Then add small difference to alpha, able to generate the largest loss among all of other examples in that row -> make the model update weights more


#### One Shot Learning

Classification vs One Shot Learning

- Classification: classify as 1 of K classes, probably use softmax function at the end to find maximum prabability. ÊØîÂ¶Çk=10, Â¶ÇÊûúkÂ¢ûÂä†‰∏Ä‰∏™, expensive to retrain the model. Besides unless you have many examples for the class. model training won't work well
- One Shot Learning: one shot learning is to be able to classify new classes without retraining any models. You only need to learn the similarity function once. Then can test similarity score against some threshold to see if they are the same. Problem changes to determine which class to measure similarity between classes
  - ÊØîÂ¶Ç‰∏ãÈù¢ÁöÑ‰æãÂ≠êÔºåÊØîÂ¶ÇÈì∂Ë°åÁ≠æÂ≠óÔºåÊØèÊ¨°ÊúâÊñ∞ÁöÑÁ≠æÂêç, can't retrain entire system. Instead, just learn a similarity  -> identify whether two signature are the same or not

![](/img/post/Natural-Language-Processing/course3/week4pic9.gif)


#### Training / Testing

1. prepare batches with batch size = b. Corresponding questions from each batch are duplicate(batch 1 Âíå batch 2Á¨¨‰∏ÄË°åÊòØduplicate). No duplicates within an indiivual batch(ÊØîÂ¶Çbatch 1  Á¨¨‰∏ÄË°åÂíåÁ¨¨‰∫åË°å‰∏çÊòØduplicate).
2. Use these inputs t get outputs vectors for each batch (Embedding -> LSTM -> Vectors -> Cosine Similarity). <span style="color:red">Note two subnetworks have identical parameters so only train one set, then shared by both</span>
   - Note: <span style="background-color:#FFFF00">$$\tau$$ and margin $$\alpha$$ from loss function are tunable hyperparameter</span>
3. When testing model, perform one-shot learning. 
  1. Convert each input an an array of numbers 
  2. Feed arrays into model 
  3. Compare $$v_1, v_2$$ using cosine similarity
  4. Test against a threshold $$\tau$$. If similarity greater than $$\tau$$, questions are classified as duplicates

![](/img/post/Natural-Language-Processing/course3/week4pic10.gif)