---
layout:     post
title:      "NLP"
subtitle:   "coursera note"
date:       2020-11-12 20:00:00
author:     "Becks"
header-img: "img/post-bg2.jpg"
catalog:    true
tags:
    - Deep Learning
    - Machine Learning
    - Â≠¶‰π†Á¨îËÆ∞
---

<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>

## Natural Language Processing with Classification and Vector Spaces

#### Week1 Vocabulary & Feature Extraction

**Sparse Representation**: 

- Represent Text as a vector. Check every word from vocabulary, if appear, note 1 else 0
- Disadvantage: Large
- Logistric regression learn **N+1** where N is the size of vocabulary
- Build vocabulary: process one by one and get sets of lists of word


```
e.g. I am happy because I am learning NLP

[I, am, happy, because, learning NLP, ... hated, the, movie]
 1   1   1       1         1       1        0     0     0

```



![](/img/post/Natural-Language-Processing/course1/week1pic1.png)

**Positive & Negative Frequencies**: Build vocabulary, map word to frequencey

![](/img/post/Natural-Language-Processing/course1/week1pic2.png)

![](/img/post/Natural-Language-Processing/course1/week1pic3.png)

![](/img/post/Natural-Language-Processing/course1/week1pic4.png)

![](/img/post/Natural-Language-Processing/course1/week1pic5.png)


<span style="background-color: #FFFF00">Faster speed for logistic regression, Instead learning V feature(size of vocabulary), only learn 3 features</span>


![](/img/post/Natural-Language-Processing/course1/week1pic6.png)

![](/img/post/Natural-Language-Processing/course1/week1pic7.png)

![](/img/post/Natural-Language-Processing/course1/week1pic8.png)


Hence you end up getting the following feature vector ```[1,8,11][1,8,11]```. 11 corresponds to the bias, 88 the positive feature, and 1111 the negative feature.


**Preprocessing**: 

When preprocessing, you have to perform the following:

1. Eliminate handles(@...) and URLs
2. Tokenize the string into words.
3. Remove stop words like  ‚Äúand, is, are, at, has, for ,a‚Äù
4. Stemming- or convert every word to its stem. Like dancer, dancing, danced, becomes 'danc'. You can use porter stemmer to take care of this.
5. Convert all your words to lower case.


![](/img/post/Natural-Language-Processing/course1/week1pic9.png)


![](/img/post/Natural-Language-Processing/course1/week1pic10.png)

matrix with m rows and three columns. Every row: the feature for each tweets

![](/img/post/Natural-Language-Processing/course1/week1pic11.png)


**General Implementation**: 

```python
freqs = build_freqs(tweets,labels) #Build frequencies dictionary
X = np.zeros((m,3)) #Initialize matrix X
for i in range(m): #For every tweet
    p_tweet = process_tweet(tweets[i]) #process tweet, include
                                      # deleting stops, stemming, deleting URLS, and handles and lower cases
    X[i, :] = extract_features(p_tweet, freqs) #Extract Features: 
                                               #By summing up the positive and negative frequencies of the tweets
                                               
```

**Logistic Regression**

$$x^{\left( i \right)} $$ is i-th tweet

![](/img/post/Natural-Language-Processing/course1/week1pic12.png)

![](/img/post/Natural-Language-Processing/course1/week1pic13.png)


**Logistic Cost Function Derivation**

$$P\left( y \mid x^{\left( i \right)}, \theta \right) = h \left(x^{\left( i \right)}, \theta \right)^{y^\left( i \right)} \left( 1-  h \left(x^{\left( i \right)}, \theta \right) \right)^{\left( 1- y^\left( i \right) \right)} $$

when y = 1, you get $$h \left(x^{\left( i \right)}, \theta \right)$$, when y = 0, you get $$ \left( 1-  h \left(x^{\left( i \right)}, \theta \right) \right) $$. In either case, you want to maximize $$ h \left(x^{\left( i \right)}, \theta \right)$$ clsoe to 1. when y = 0, you want $$ \left( 1-  h \left(x^{\left( i \right)}, \theta \right) \right)  $$ to be 0  as $$ h \left(x^{\left( i \right)}, \theta \right)$$ close to 1, when y = 1, you want $$ h \left(x^{\left( i \right)}, \theta \right) = 1$$

Define the likelihood as 

$$L\left(  \theta \right) = \prod_{i=1}^{m}  h \left(\theta, x^{\left( i \right)} \right)^{y^\left( i \right)} \left( 1-  h \left(\theta, x^{\left( i \right)} \right) \right)^{\left( 1- y^\left( i \right) \right)}   $$


![](/img/post/Natural-Language-Processing/course1/week1pic14.png)


**Gradient Descent Derivation**

![](/img/post/Natural-Language-Processing/course1/week1pic15.png)

![](/img/post/Natural-Language-Processing/course1/week1pic16.png)

![](/img/post/Natural-Language-Processing/course1/week1pic17.png)


#### Week2 Naive Bayes

Tweet <span style="color:red">defined either positive or negative, cannot be both</span>. Bayes' rule is based on the mathmatical formulation of conditional probabilities. <span style="background-color: #FFFF00">Called **Naive** because make assumpton that the **features you're using for classification are all independent**, which in reality is rare</span>

$$ P\left(X \mid Y \right) = \frac{ P\left(Y \mid X \right) P\left(X \right)}{P\left(Y \right)} $$

e.g. Happy sometimes as positive word sometimes as negative, A the number of positive tweet, B the number tweet contain happy

![](/img/post/Natural-Language-Processing/course1/week2pic1.png)

$$ P\left(Positive \mid happy \right) = \frac{ P\left(Positive \cap happy \right)}{P\left(happy \right)} $$

$$ P\left(happy \mid Positive \right) = \frac{ P\left(Positive \cap happy \right)}{P\left(Positive \right)} $$

$$ P\left(Positive \mid happy \right) =  P\left(happy \mid Positive \right) \frac{ P\left(Positive \right)}{P\left(happy \right)} $$

Q: Suppose that in your dataset, 25% of the positive tweets contain the word ‚Äòhappy‚Äô. You also know that a total of 13% of the tweets in your dataset contain the word 'happy', and that 40% of the total number of tweets are positive. You observe the tweet: ''happy to learn NLP'. What is the probability that this tweet is positive? <br/>
A: 0.77

![](/img/post/Natural-Language-Processing/course1/week2pic2.png)

- Word with equal probability for positive/negative don't add anything to the sentiment (e.g. I, am, learning, NLP) 
- "sad, happy, not" have a significant difference between probabilities. 
- "because" negative class is zero, no way of comparing between two corpora, a problem

![](/img/post/Natural-Language-Processing/course1/week2pic3.png)


Once you have the probabilities, you can compute the likelihood score as follows. A score greater than 1 indicates that the class is positive, otherwise it is negative.


![](/img/post/Natural-Language-Processing/course1/week2pic4.png)


**Laplacian Smoothing**

We usually compute the probability of a word given a class as follows:

$$ P\left(w_i \mid class \right) = \frac{freq\left(wi,class \right)}{N_{class}} \quad \text{class} \in \text{\{Positive, Negative\}}$$

- $$freq_{positive}$$ and $$freq_{negative}$$  are the frequencies of that specific word in the positive or negative class. In other words, the positive frequency of a word is the number of times the word is counted with the label of 1.
- $$N_{positive}$$ and $$N_{negative}$$  are the total number of positive and negative words for all documents (for all tweets), respectively.


However, if a word does not appear in the training, then it automatically gets a probability of 0, to fix this we add smoothing as follows

$$ P\left(w_i \mid positive \right) = \frac{freq\left(wi,positive \right) + 1}{N_{positive} + V}$$

$$ P\left(w_i \mid negative \right) = \frac{freq\left(wi,negative \right) + 1}{N_{negative} + V}$$

Note that we added all in the numerator, and since there are V words to normalize, we add V in the denominator. where 


<span style="color:red">$$V$$:  the number of unique words in the entire set of documents, for all classes, whether positive or negative.</span>


![](/img/post/Natural-Language-Processing/course1/week2pic5.png)

**Log Likelihood**

![](/img/post/Natural-Language-Processing/course1/week2pic6.png)


To compute the log likelihood, we need to get the ratios and use them to compute a score that will allow us to decide whether a tweet is positive or negative.  <span style="background-color: #FFFF00">The higher the ratio, the more positive the word is</span>. To do inference, you can compute the following:



$$ \frac{P\left(pos\right)}{P\left(neg\right)} \prod_{i=1}^m \frac{P\left(w_i \mid pos\right)}{P\left(w_i \mid neg\right)} > 1 $$



m gets larger, we can get numerical flow issues, hard to store on device , so we introduce the log, which gives you the following equation 

$$ log\left( \frac{P\left( D_{pos}\right)}{P\left(D_{neg}\right)} \prod_{i=1}^m \frac{P\left(w_i \mid pos\right)}{P\left(w_i \mid neg\right)} \right) =>  \underbrace{log\frac{P\left(D_{pos}\right)}{P\left(D_{neg}\right)}}_{\text{log prior}} + \underbrace{\sum_{i=1}^m log \frac{P\left(w_i \mid pos\right)}{P\left(w_i \mid neg\right)}}_{\text{log likelihood}} $$

$$\text{where} \quad P\left(D_{pos}\right) = \frac{D_{pos}}{D},  P\left(D_{neg}\right) = \frac{D_{neg}}{D}$$
 
 <span style="background-color: #FFFF00">where D is the total number of documents, $$D_{pos}$$ is the total number of positive tweets and $$D_{neg}$$ is the total number of negative tweets.</span> <span style="color:red">Ê≥®Ôºö‰∏é$$N_{pos}$$ ‰∏çÂêåÔºå $$N_{pos}$$ ÊòØtotal number of all positive words</span>
 
<span style="background-color: #FFFF00">log prior represents the underlying probability in the target population that a tweet is positive versus negative.  important for unbalanced dataset</span>. We further introduce Œª as follows:

![](/img/post/Natural-Language-Processing/course1/week2pic7.png)

<span style="color:red">The positive value indicates tweet is positive. Negative value indicates tweet is negative</span>

![](/img/post/Natural-Language-Processing/course1/week2pic8.png)

**Training Naive Bayes**

1. collect and annotate corpus: divide tweets into two groups, positive/negative
2. Preprocess the tweets: process_tweet(tweet) ‚ûû [w1, w2, w3, ...]:
   - Lowercase
   - Remove punctuation, urls, names
   - Remove stop words
   - Stemming
   - Tokenize sentences
3. Compute freq(w, class), use below table to compute
4. Get $$P\left(w \mid  pos \right), P\left(w \mid neg \right)$$ using Laplacian Smoothing formula
5. Get $$\lambda\left( w \right) =  log \frac{P\left( w \mid pos \right)}{P\left( w \mid neg \right)}$$
6. Compute $$logprior = log \frac{D_{pos}}{D_{neg}} $$ where $$ D_{pos} $$ and $$D_{neg}$$ correspond to  the number of positive and negative documents respectively


![](/img/post/Natural-Language-Processing/course1/week2pic9.png)


**Testing Naive Bayes**

if a word not in vocabulary, think as neutral, not contribute to score

![](/img/post/Natural-Language-Processing/course1/week2pic10.png)

$$\text{Accuracy} \rightarrow \frac{1}{m} \sum_{i=1}^{m} {pred_i == Y_{val}} $$


**Naive Bayes Application**

There are many applications of naive Bayes including:

- Author identification: document written by which author, calculate $$\lambda$$ for each word for both author to build vocabulary
- Spam filtering: $$\frac{P\left(spam \mid email \right)}{P\left(nnspam \mid email \right)}$$
- Information retrieval: filter relevant and irrelevant documents in a database, calculate the likelihood of the documents given the query, retrieve document if $$P\left(document_k \mid query \right) \approx \prod_{i=0}^{\mid query \mid} P\left(query_i \mid document_k  \right) > threshold$$
- Word disambiguation: e.g. don't know bank in reading, refer to river or money? to calculate the score of the document, $$\frac{P\left(river \mid text\right)}{P\left(money \mid text\right)}$$. if refer to river not money, score is bigger than 1. 


**Naive Bayes Assummtion**

<span style="background-color: #FFFF00">Advantage: simple doesn't require setting any custom parameters</span>.

Assumption:

1. <span style="background-color: #FFFF00">Indepdence between predictors or features associated with each class</span>
  - assume word in a piece of text are independent. e.g. "It is sunny and hot in the Sahara desert." But "sunny" and "hot" often appear together. And together often related to beach or desert. Not always independent 
  - <span style="color:red">Under or over estimate conditional probabilities of individual words</span>
  - e.g. "It's always cold and snowy in ___" naive model will assign equal weight to the words "spring, summer, fall, winter".
2. Relative frequencies in corpus: <span style="background-color: #FFFF00">rely on distribution on training dataset</span>
   - A good dataset contain the same proportion of positive and negative weets as a random sample. But in reality positive tweet is more common(because negative tweets maybe muted or banned by twitter). This would result a very optimistic or pessimistic model


**Error Analysis**

1. Semantic meaning lost in the pre-processing step by removing punctuation and stop words
   - "My beloved grandmother :(", with punctuation indicating a sad face, after process, [belov, grandmoth], positive tweet
   - "This is not good, because your attitude is not even close to being nice." if remove neutral word, [good, attitude, close, nice] positive
2. Word order affect meaning
   - "I am happy because I did not go." vs. "I am not happy because I did go."
3. Adversarial attack: some quirks of languages come naturally to human but confuse Naive Bayes model
   - some common language phenomenon, Sarcasm, Irony and Euphemisms.
   - e.g. "This is a ridiculously powerful movie. The plot was gripping and I cried right through until the ending" positive. After process. [ridicul, power, movi, plot, grip, cry, end] negative



#### Week3 Vector Space

e.g. "Where are you heading" vs "Where are you from" -> Different meaning Â∞ΩÁÆ°Ââç‰∏â‰∏™ËØç‰∏ÄÊ†∑<br/>
e.g. "What is your age" vs "How old are you" -> Same meaning

e.g. "You eat ceral from  a bowl" ceral and bowl are related <br/>
e.g. "You can buy something and someone else sells it" second half sentence is dependent on the first half

- Vector space: <span style="background-color: #FFFF00">identify the context around each word  to capture relative meaning, to represent words and documents as vector</span>
- Vector space allow to identify sentence are similar even if they do not share the same words
- Vector space allow to capture dependencies between words
- application:
  -  information extraction to answer questions in the style of who, what, where, how... 
  -  machine translation
  -  chatbots programming: Êú∫Âô®ÂÆ¢Êúç


**Word by Word Design**

number of times they <span style="background-color: #FFFF00">occur together within a certain **distance** k</span>


 <span style="background-color: #FFFF00">Co-occurence matrix</span>: e.g. data =2, count how many appearance for  each word has distance smaller or euqal than 2


![](/img/post/Natural-Language-Processing/course1/week3pic1.png)

<span style="color:red">Word by word , get n entries, with n between 1 and size(vocabulary)</span>



**Word by Document Design**

number of times they <span style="background-color: #FFFF00">occur together within a certain **category** k</span>

Divide corpus into different tops like Entertainment, Economy, Machine Learning. Below data appear 500 times in corpus related to Entertainment, 6620 times in documents related to Economy

![](/img/post/Natural-Language-Processing/course1/week3pic2.png)

![](/img/post/Natural-Language-Processing/course1/week3pic3.png)

**Euclidean Distance**

Euclidean Distance is the norm of the difference between vectors, straight line between points

$$d\left( \vec{v}, \vec{w} \right) = \sqrt{\sum_{i=1}^n \left(v_i - w_i \right)^2 } \quad \rightarrow \quad \text{Norm of }  \left( \vec{v}, \vec{w} \right) $$

![](/img/post/Natural-Language-Processing/course1/week3pic4.png)

```python
v = np.array([1,6,8])
w = np.array([0,4,6])

#Calculate the Eudlidean distance d 

d = np.linalg.norm(v-w)
```


**Cosine Similarity**

Problem with Euclidean distance:  Vector space where corpora are represented by occurrence of the words "disease" and "eggs". Because Agriculture and history corpus has a similar number of words(Âá∫Áé∞ÂçïËØçÊï∞Èáè‰∏ÄÊ†∑), Suggests agriculture and history ÊØî agriculture and food Êõ¥ÊÉ≥Ëøë‰πâ

- if A and B are identical, you will get $$cos\left( \theta \right)=1$$
- If you get $$cos\left( \theta \right)=0$$, that means that they are orthogonal (or perpendicular).
- Numbers between 0 and 1 indicate a similarity score.
- Numbers between -1-0 indicate a dissimilarity score.
- <span style="background-color: #FFFF00">Advantage: Cosine similarity isn't biased by the size difference between the representations</span>

![](/img/post/Natural-Language-Processing/course1/week3pic5.png)

$$\begin{align} \hat{v} \cdot \hat{w} &= \Vert \hat{v} \Vert \Vert \hat{w} \Vert cos \left( \beta \right) \\  cos \left( \beta \right) &= \frac{\hat{v} \cdot \hat{w}}{\Vert \hat{v} \Vert \Vert \hat{w} \Vert}  \\ &= \frac{\left(20 \times 30\right) + \left(40 \times 20 \right)}{\sqrt{20^2 + 40^2} \times \sqrt{30^2 + 20^2}}\end{align}$$

-  proportional to the similarity between the directions of the vectors that you are comparing. 
-  the cosine similarity values between 0 and 1 for word by Word/Docs, cosine similarity values between -1 and 1 when using word embedding 

![](/img/post/Natural-Language-Processing/course1/week3pic6.png)

**PCA**

- PCA: dimenison reduction from high dimension to fewer dimension. project data into a lower dimension while retaining as much information as possible
- Original Space -> Uncorrelated features -> Dimension Reduction
- Can use visualization to check if PCA capure relationship among words
- <span style="color:red">**Eigenvector**</span>: <span style="background-color: #FFFF00">directions</span> of uncorrelated features for your data
- <span style="color:red">**Eigenvalue**</span>: amount of information retained by each feature(<span style="background-color: #FFFF00">variances</span> of data in each of those new features)
- <span style="color:red">**Dot Product**</span>: gives the <span style="background-color: #FFFF00">projection</span> on uncorrelated features

![](/img/post/Natural-Language-Processing/course1/week3pic7.png)

1. Mean Normalize Data  $$x_i = \frac{x_i - \mu_i}{\sigma_{x_{i}}}$$
2. Get Covariance Matrix
3. Perform Singular Value Decomposition to get set of three matrices(Eigenvectors on column wise, Eigenvalues on diagonal). <span style="background-color: #FFFF00"> Eigenvalues and eigenvectors should be organized according to the eigenvalues in descending order. Ensure to retain as much as information as possible </span>
4. Project data to a new set of features, using the eigenvectors and eigenvalues in this steps


![](/img/post/Natural-Language-Processing/course1/week3pic8.png)

![](/img/post/Natural-Language-Processing/course1/week3pic9.png)



#### Week4 Transforming word vectors

Translate word: caculate word embedding in English and French, transform word embedding in English to French and find the most similar one

![](/img/post/Natural-Language-Processing/course1/week4pic1.png)

R: transform matrix, X: English word, Y: French word.  First get subsets of English word and French equivalence

![](/img/post/Natural-Language-Processing/course1/week4pic2.png)

<span style="color:red">Why not store all vocabulary?</span>: just need a subset to collect to find transform matrix. If it works well, then model can translate words that are not in vocabulary

**Solving for R**:

$$ \begin{align} \color{olive}{\text{Initialize R}}\quad & \\
 \color{blue}{\text{in a loop:}}\quad & \\ 
 & Loss = \Vert XR - Y \Vert_F  \qquad   \color{fuchsia}{\text{Frobenium Norm}} \\
 & g = \frac{d}{dR} Loss \qquad   \color{fuchsia}{\text{gradient R}}  \\
 & R = R - \alpha g \qquad \quad   \color{fuchsia}{\text{update}} 
\end{align} $$ 


Can pick a fixed number of times to go through the loop or check the loss at each iteration and break out loop when loss falls between a certain threshold.


<span style="background-color: #FFFF00">Explanation for fixed number of iterations iterating until the loss falls below a threshold.</span>

- You cannot rely on training loss getting low -- what you really want is the validation loss to go down, or validation accuracy to go up. And indeed - in some cases people train until validation accuracy reaches a threshold, or -- commonly known as "early stopping" -- until the validation accuracy starts to go down, which is a sign of over-fitting.
- Why not always do "early stopping"? Well, mostly because well-regularized models on larger data-sets never stop improving. Especially in NLP, you can often continue training for months and the model will continue getting slightly and slightly better. This is also the reason why it's hard to just stop at a threshold -- unless there's an external customer setting the threshold, why stop, where do you put the threshold?
- Stopping after a certain number of steps has the advantage that you know how long your training will take - so you can keep some sanity and not train for months. You can then try to get the best performance within this time budget. Another advantage is that you can fix your learning rate schedule -- e.g., lower the learning rate at 10% before finish, and then again more at 1% before finishing. Such learning rate schedules help a lot, but are harder to do if you don't know how long you're training.

**Frobenius norm**


$$ \Vert A \Vert_F = \sqrt{\sum_{i=1}^m \sum_{j=1}^n \vert a_{ij}\vert^2 } $$ 

e.g. has two word embedding and dimension is two, the matrix is 2x2

$$ A = \begin{pmatrix}
    2 & 2\\
    2 & 2\\
  \end{pmatrix}$$

$$  \Vert A \Vert_F = \sqrt{2^2 + 2^2 + 2^2 + 2^2} = 4 $$

```python
A = np.array([[2,2,],
              [2,2,]])
A_squared = np.square(A)
print(A_squared)  # array([[4,4,],

                  #        [4,4,]])

A_Frobenious = np.sqrt(np.sum(A_squared))
```

**Frobenius norm squared**

$$\frac{1}{m} \Vert XR - Y \Vert_F^2 $$

- The norm is always nonnegative (we're summing up absolute values), and so is the square.
- When we take the square of all non-negative (positive or zero) numbers, the order of the data is preserved. For example, if 3 > 2, 3^2 > 2^2
- Using the norm or squared norm in gradient descent <span style="color:red">results in the same location of the minimum</span>.
- Squaring cancels the square root in the Frobenius norm formula. Because of the chain rule, we would have to do more calculations if we had a square root in our expression for summation.
- Dividing the function value by the positive number doesn't change the optimum of the function, for the same reason as described above.
- divide by m: We're interested in transforming English embedding into the French. Thus, it is more important to measure average loss per embedding than the loss for the entire dictionary (which increases as the number of words in the dictionary increases).



so the gradient will be: 

**Gradient**

Easier to take derivative rather than dealing with the square root in Frobenius norm

$$  Loss = \Vert XR - Y \Vert_F^2  $$

$$  g = \frac{d}{dR}Loss = \frac{2}{m} X^T \left( XR - Y  \right) $$



**Locality Sensitive Hashing**

<span style="background-color: #FFFF00">Reduce computational cost(much faster) to find K-nearest neighbor </span>is to use locality sensitive hashing. For example, using word vectors with just two dimensions. **plan** can helps bucket vectors into subsets based on localtion(ËìùËâ≤ÁÇπÂú®ËìùËâ≤plane ‰∏äÊñπÔºåÁÅ∞Ëâ≤ÁÇπÂú®ÁÅ∞Ëâ≤plane‰∏äÊñπ)

![](/img/post/Natural-Language-Processing/course1/week4pic3.png)

normal vector: perpendicular to any vectors on the plane.

![](/img/post/Natural-Language-Processing/course1/week4pic4.png)

If dot product positive, it's on one side of the plane. If dot product negative, it's on opposite side of the plane. If dot product is zero, it is on plane

dot product of P and V1 is equal to the do length of P multiply V1 projection onto P

![](/img/post/Natural-Language-Processing/course1/week4pic5.png)

![](/img/post/Natural-Language-Processing/course1/week4pic6.png)

```python
def side_of_plane(P,v):
  dotprodcut = np.dot(P,v.T)
  sign_of_dot_product = np.sign(dotproduct)
  sign_of_dot_prodcut_scalar = np.asscalar(sign_of_dot_product)
  return sign_of_dot_prodcut_scalar
```

**Multiple Plane**

To divide vector space into manageable regions, want to use more than one plane. Get multiple signal, one for each plane, but need a single hash value 


![](/img/post/Natural-Language-Processing/course1/week4pic7.png)

Rule: 

$$  \color{maroon}{\text{sign_i} \geq 0, \rightarrow h_i = 1}  $$

$$  \color{maroon}{\text{sign_i} < 0, \rightarrow h_i = 0 } $$

$$  \mathbf{hash = \sum_i^H 2^i \times h_i} $$

```python
def hash_multiple_plane(P_l, v):
    hash_value = 0
    for i, P in enumerate(P_l):
        sign = side_of_plane(P,v)
        hash_i = 1 if sign >=0 else 0
        hash_value += 2**i * hash_i
    return hash_value
```



```python
#Generate Random Plane

num_dimensions = 2 
num_planes = 3

random_planes_matrix = np.random.normal(
                       size = (num_planes,
                               num_dimensions)) 
#find out whether vector v is on positive or negative side of each planes

def side_of_plane_matrix(P,v):
  dotproduct = np.dot(P,v.T
  sign_of_dot_product = np.sign(dotproduct)
  return sign_of_dot_product

v = np.array([[2,2,]])
num_planes_matrix = side_of_plane_matrix(random_planes_matrix, v)
```


Text can be embedded into vector space so that nearest neighbors refer to text with similar meaning

![](/img/post/Natural-Language-Processing/course1/week4pic8.png)



## Natural Language Processing with Probabilistic Models

#### Week1 Autocorrect and Minimum Edit Distance


How it works?

1. Identify a misspelled word(only spelling error, not contextual error(e.g. Happy birthday to deer friend))   - how ? if spelled correctly, it is in dictionary. if not, probably a misspelled word
2. Find strings n edit distance way  e.g. deah vs dear
   - Edit: an operation perfomed on a string to change it, including: insert, delete, switch (eta - > eat, eta -> tea, but not eta -> ate), replace
   - Autocorrect: n is usually 1 to 3 edits
3. Filter candidates 
   - step 2 generate word if not in vocabulary, remove it. e.g. deah -> d_ar, if d_ar not in vocabulary remove it
4. Calculate word probabilities

![](/img/post/Natural-Language-Processing/course2/week1pic1.png)

Minimum distance application: spelling correction, document similarity, machine translation, DNA sequencing etc

Minimum edit distance(**Levenshtein distance**): insert(cost 1), delete(cost 1), <span style="background-color: #FFFF00">replace(cost 2, can think of delete then insert)</span>

![](/img/post/Natural-Language-Processing/course2/week1pic2.png)


#### Week2 Part of Speech Tagging and Hidden Markov Models

![](/img/post/Natural-Language-Processing/course2/week2pic1.png)

[Meaning of Tag](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)

Application: 

- indentify entity: "Effel Tower is in Paris"  Effel Tower & Paris are entities
- Co-reference resolution: "Effel Tower is in Paris, it is 324 meters high", speech tagging indentify it refer to Eiffel Tower
- Speech Recognition: check if a sequence of word has a high probability or not


**Markov chain**: type of stochastic model that describes a sequence of possible events. To get probability of each event, it needs only the states of the previous events. Stochastic means random or randomness. Markov chain can be depicted as <span style="background-color: #FFFF00">directed graph</span>. A state refers to a certain condition of the present moment.

e.g. Why not learn ... (next is verb? or norm?) depends on previous word

![](/img/post/Natural-Language-Processing/course2/week2pic2.png)

To predict the first word, because state depend on previous state, introduce initial state. Transition Matrix A (n+1) x n where n is the number of hidden states.

![](/img/post/Natural-Language-Processing/course2/week2pic3.png)


**Hidden Markov Model**:

For human, can recognize fly = verb, but machine cannot. Observable: can only seen by the machine

![](/img/post/Natural-Language-Processing/course2/week2pic5.png)


<span style="color:red">**Emission probability**</span>: describe the transition from hidden states of hidden Markov model(Norn, verb...) to the words of corpus

![](/img/post/Natural-Language-Processing/course2/week2pic6.png)

e.g. VB -> eat 0.5 means:  when model is currently at hidden state for a verb, 0.5 chance that observable the model will emit the word eats.  ÂèØ‰ª•Ê≥®ÊÑèÂà∞ÊØè‰∏™word ÁöÑtagÁöÑÊ¶ÇÁéáÈÉΩÂ§ß‰∫é0ÔºåÂõ†‰∏∫context ‰∏çÂêåÔºåtag‰∏çÂêå, ‰æãÂ¶Ç He lay on his <span style="color:red">*back*</span> V.S. I'll be <span style="color:red">*back*</span>


![](/img/post/Natural-Language-Processing/course2/week2pic7.png)



![](/img/post/Natural-Language-Processing/course2/week2pic8.png)

**Calculate Probability**:


![](/img/post/Natural-Language-Processing/course2/week2pic9.png)


Process Corpus:

1. At start token in each line of sentence as to calculate initial probability
2. transform all words in lowercase and keep punctuation 


![](/img/post/Natural-Language-Processing/course2/week2pic10.png)

‰∏∫‰∫ÜÈÅøÂÖçprobability as 0, add a small value eplison(e.g. 0.001)

![](/img/post/Natural-Language-Processing/course2/week2pic11.png)

Since vb is 1/3 for all outgoing transitions. ÊòØReasonableÁöÑ because don't have any data to estimate these probabilities

In reall world, don't apply smoothing for initial probability. Because: if add, allow a sentence to start with any parts of speech tag, including punctuation

![](/img/post/Natural-Language-Processing/course2/week2pic12.png)


**Populating the Emission Matrix**

![](/img/post/Natural-Language-Processing/course2/week2pic13.png)


$$ C\left(t_i \right) $$: tag i ÊÄªÂÖ±Âá∫Áé∞ÁöÑÊ¨°Êï∞Ôºå<br/>
$$ C\left(t_i, w_i \right) $$ word_i Â±û‰∫étag_i Âá∫Áé∞ÁöÑÊ¨°Êï∞

![](/img/post/Natural-Language-Processing/course2/week2pic14.png)


**Viterbi Algorithm**: a graph algorithm

from 0 -> NN / VB has the same probability. But the emission probability of love is higher for VB. From love(VB) -> to(O), Ê≤°ÊúâÂÖ∂‰ªñÁöÑtagÊúâemission probability of to

![](/img/post/Natural-Language-Processing/course2/week2pic15.gif)

- Matrix C: hold ther intermediate optimal probabilities 
  - first column C: the probability from start state $$\pi$$ to the first tag $$t_i$$ and  word $$W_1$$. $$  c_{i,1} = a_{1,i} \times b_{i,cindex\left(w_1\right)}$$: product of transition probability(from 1 to tag i) and emission probability(from tag i to word1)
- Matrix D: The most likely indices of visited state
  - The column is all 0, no proceeding tags traversed
- Both matrix <span style="color:red">n rows: the number of speech tags/hidden states. K columns: the number of words</span>

![](/img/post/Natural-Language-Processing/course2/week2pic16.gif)


**Forward Pass**

*Matrix C*

$$  c_{i,j} = \underset{k}{max} \ c_{k,j-1} \times a_{k,i} \times  b_{i,cindex\left(w_j\right)} \text{}$$


- <span style="background-color: #FFFF00">$$b_{i,cindex\left(w_j\right)}$$: emission proability from $$tag_i$$ to $$word_j$$</span>
- <span style="background-color: #FFFF00">$$a_{k,i}$$ the transition probability from previous $$tag_k$$ to current $$tag_i$$</span>
- <span style="background-color: #FFFF00">$$c_{k,j-1} $$: the probability of proceeding path traversed</span> Âà∞ tag k, Á¨¨j-1 ‰∏™textÊúÄ‰Ω≥ÁöÑÊ¶ÇÁéá
- Then choose k to maximize the entire formula

In real word use <span style="color:red">**log probability**: to avoid multiply very small number, may lead to numeric issue for multiplication </span>

$$  c_{i,j} = \underset{k}{max} log\left(\ c_{k,j-1}\right) + log\left(a_{k,i}\right) +  log\left(b_{i,cindex\left(w_j\right)}\right) \text{}$$

*Matrix D*

$$  d_{i,j} = \underset{k}{argmax} \ c_{k,j-1} \times a_{k,i} \times  b_{i,cindex\left(w_j\right)} \text{}$$

- in each $$  d_{i,j}$$  save the k which maximize the entry in $$ c_{i,j}$$
- argmax returns k which maximize the function arguments instead of maximum value

In the below example, three states are not initial state, state can be 1,2,3. k can be 1,2,3


![](/img/post/Natural-Language-Processing/course2/week2pic17.gif)


**Backward Pass**

Extract the graph from matrix D, which represents most likely sequence of hidden states:  Get the index of entry of $$C_{i,k}$$ which is the highest probability in the last column C. Use that index s ($$\underset{i}{argmax} \ c_{i,k}$$) to traverse backwards through the matrix D to reconstruct the sequence. ‰ªéÂêéÂæÄÂâçÊé®

e.g. ‰∏ãÈù¢ÁöÑ‰æãÂ≠ê,

1. ÊúÄÂêé‰∏ÄÂàóÁöÑÊúÄÂ§ßÁöÑÊ¶ÇÁéáÊòØ  $$C_{1,5} = 0.01$$, $$s = \underset{i}{argmax} \ c_{i,k} = 1$$. The most likely tag for word5 is tag1
2. $$D_{1,5} = 3$$, so previous index is 3 ( tag 3 is the pervious state with the highest proabability for $$C_{1,5}$$)
3. Algorithm stops as arrived at the start token

![](/img/post/Natural-Language-Processing/course2/week2pic18.gif)

[Forward & Backward Example](https://github.com/beckswu/Natural-Language-Processing/blob/master/Coursera/Course2-Natural%20Language%20Processing%20with%20Probabilistic%20Models/week2-Assignment-%20Part%20of%20Speech%20Tagging.ipynb)

#### Week3 Autocomplete and Language Models

Language model: calculate the probability of sentences

- Estimate probability of word sequences. 
- Estimate probability of a word following a sequence of words with most likely suggestions(**Autocomplete a sentence**) e.g. if you type, how are, e-mail application guess you

Application:

- Speech recognition:  P(I saw a van) > P(eyes awe of an)
- Spelling correction
- Augmentatitve communication: predict most likely word from menu for people unable to physically talk or sign
  - Augmentative communication system: take a series of hand gestures from a user to help them form words and sentences


**N-gram**: 

- An N-gram: sequence of N words. Order matters
- punctuation is treated as words. But all other special characters such as codes will be removed


E.g. 
- Bigrams are all sets of two words that appear side by side in corpus. <span style="color:red">Notes: ‰∏ãÈù¢‰æãÂ≠ê, I am appear twice, ‰ΩÜÊòØonly include ‰∏ÄÊ¨°Âú® bigram set</span>, I happy Âõ†‰∏∫‰∏çÁõ∏ÈÇªÔºåÊâÄ‰ª•‰∏çÊòØ
- Tigram: represent unique triplets of words that appear together in the sequence together in the corpus


$$ \text{Corpus: I am happy because I am learning} $$

$$ \text{Unigrams: \{I, am, happy, because,learning\}} $$

$$ \text{Bigrams: \{I am, am happy, happy because,...\}},note: \color{red}{\text{"I happy" not in bigram}  } $$

$$ \text{Bigrams: \{I am happy, am happy because, ...\}} $$

Sequence Notation:

$$W_1^3: \text{sequence of wrods from word 1 to word 3}$$

$$W_{m-2}^m: \text{sequence of wrods from word m-2 to word m}$$

![](/img/post/Natural-Language-Processing/course2/week3pic1.png)

**Probability** 

Note: in bigram: $$ P\left( happy \mid I \right) = 0$$ Âõ†‰∏∫ I happy never appear in the corpus

$$ \text{Probabiity of bigram} \quad P\left( y \mid x \right) = \frac{ C\left(x, y \right)}{\sum_w{C\left(x, w \right)}} = \frac{ C\left(x y \right)}{C\left(x \right)}$$

$$\text{where }C_\left(x \right): \text{the count of all unigram x.} \quad \sum_w{C\left(x, w \right)}: \text{the count of all bigram starting with x}$$

$$\sum_w{C\left(x, w \right)}$$ bigram can be simpilied to $$C_\left(x \right)$$ unigram: <span style="color:red">only works if x is followed by another word</span>

$$ \text{Probabiity of trigram} \quad P\left( w_3 \mid w_1^2 \right) = \frac{ C\left(w_1^2 w_3\right)}{C\left(w_1^2\right)}$$

$$C\left(w_1^2 w_3\right) = C\left(w_1 w_2 w_3 \right) \quad \text{a bigram followed by a unigram}$$

$$ \text{Probabiity of N-gram} \quad P\left( w_N \mid w_1^{N-1} \right) = \frac{ C\left(w_1^{N-1} w_N\right)}{C\left(w_1^{N-1}\right)}$$

![](/img/post/Natural-Language-Processing/course2/week3pic2.gif)

$$ \text{Chain Rule} \quad P\left(A,B,C,D\right) = P\left(A \right)P\left(B \mid A \right)P\left( C \mid\ A,B \right)P\left(D\mid A, B, C \right)$$

$$P\left(the \ teacher \ drinks \ tea\right) = P\left(the \right)P\left(teacher \mid the \right)P\left( drinks \mid\ the, teacher \right)P\left(tea\mid the, teacher, drinks \right)$$

**Problem with above calculation**: <span style="background-color: #FFFF00">Corpus almost never contains the exact sentences we're interested in or even its longer subsequences</span>, ÊØîÂ¶Ç$$C\left( the \ teacher \ drinks \ tea \right)$$ and $$C\left( the \ teacher \ drinks  \right)$$ neither of them are likely in training corpus, the count are 0. <span style="color:red">ÂΩìsentences get longer and longer, the likelihood that more and more words will occur next to each other in the exact order gets smaller and smaller</span>



<span style="background-color: #FFFF00">Solution: use approximation: **product of probabilities of bigrams**</span>

$$P\left(the \ teacher \ drinks \ tea\right) \approx P\left(the \right)P\left(teacher \mid the \right)P\left( drinks \mid teacher \right)P\left(tea\mid drinks \right)$$


- Markove assumption: only last N words matter
- Bigram: $$P\left( w_n \mid w_1^{n-1} \right) \approx P\left( w_n \mid w_{n-1} \right) $$
- N-gram: $$P\left( w_n \mid w_1^{n-1} \right) \approx P\left( w_n \mid w_{n-N+1}^{n-1} \right) $$
- Entire sentence modeled with bigram: $$P\left( w_1^n \right) \approx \prod_{i=1}^{n} P\left( w_i \mid w_i-1 \right) $$
  - contrast with Naive Bayes, approximate sentence probability without cnsidering any word history
  - $$P\left( w_1^n \right) \approx P\left(w_1 \right)P\left(w_2 \mid w_1 \right)P ... P\left(w_n\mid w_{n-1} \right) $$: The first term of Bigram reiles on unigram probability of first word n sentence

![](/img/post/Natural-Language-Processing/course2/week3pic3.gif)


**Start & End of Sentences**:

Start: 

- Problem: first word, don't have previous word, can't calculate bigram probability for first word
- Solution: add a special term ```<s>``` so can calculate the probabilities
- For n-grams, first N words, don't have enough context, <span style="background-color: #FFFF00">add n-1 special term</span>

![](/img/post/Natural-Language-Processing/course2/week3pic4.gif)

End: 

Problem 1:   $$ \text{Probabiity of bigram} \quad P\left( y \mid x \right) = \frac{ C\left(x, y \right)}{\sum_w{C\left(x, w \right)}} = \frac{ C\left(x y \right)}{C\left(x \right)}, {\sum_w{C\left(x, w \right)}$$ the count of all bigram starting with X. <span style="color:red">When x is last word of sentences, doesn't work</span>

e.g. Corpus:

- ```<s> Lyn drinks chocolate```
- ```<s> John drinks```

$$\sum_w{C\left(drinks \ w \right)} = 1, C\left(drinks\right) = 2$$

Solution: add end of sentence token


Problem 2: 

e.g. Corpus:

- ```<s> yes no```
- ```<s> yes yes```
- ```<s> no no```

all possible sentences of lengths 2:  add all probability will equal to 1  üëå 

- ```<s> yes yes```,  $$P\left( <{s}> yes \ yes \right) = \frac{1}{3}$$
- ```<s> yes no```, $$P\left( <{s}> yes \ no \right) = \frac{1}{3}$$
- ```<s> no no```   $$P\left( <{s}> no \ no \right) = \frac{1}{3}$$
- ```<s> no yes``` $$P\left( <{s}> no \ yes \right) = 0$$

all possible sentences of lengths 3: add all probability will equal to 1 üëå 

- ```<s> yes yes yes```,  $$P\left( <{s}> yes \ yes \right) = ...$$
- ```<s> yes no no```, $$P\left( <{s}> yes \ no \right) = ...$$
- ```<s> no no no``` $$P\left( <{s}> no \ yes \right) = ...$$

However, What you want is the sum of all sentences of all length to 1: $$\sum_{2 word}P\left( ...\right) + \sum_{3 word}P\left( ...\right) ...  = 1$$

**Solution**: add end of sentence token ```</s>```  

Bigram:

- ```<s> Lyn drinks chocolate </s>```
- ```<s> John drinks </s>```

$$\sum_w{C\left(drinks \ w \right)} = 2, C\left(drinks\right) = 2$$


e.g. Bigram:

```<s> the teacher drinks tea``` => ```<s> the teacher drinks tea </s>``` 

$$P\left(the | <{s}> \right)P\left(teacher \mid the \right)P\left( drinks \mid teacher \right)P\left(tea\mid drinks \right)P\left(<{/s}>| tea \right)$$

e.g. Bigram:

- ```<s> Lyn drinks chocolate </s>``` $$P\left( sentence \right) = \frac{2}{3} * \frac{1}{2} * \frac{1}{2} * \frac{2}{2} = \frac{1}{6}$$
- ```<s> John drinks tea </s>```  $$P\left( sentence \right) = \frac{1}{3} * \frac{1}{1} * \frac{1}{2}* \frac{1}{1}  = \frac{1}{6}$$
- ```<s> Lyn eats chocolate </s>``` $$P\left( sentence \right) = \frac{2}{3} * \frac{1}{2} * \frac{1}{1} *  \frac{2}{2} = \frac{1}{3}$$

$$P\left( John \mid <{s}> \right) = \frac{1}{3},P\left( <{/s}> \mid tea  \right) = 1 $$

$$P\left( chocolate \mid eats \right) = \frac{1}{3},P\left( lyn \mid <{s}>  \right) = \frac{2}{3} $$



N-gram: => <span style="background-color: #FFFF00">just one ```</s>``` at the end of sentences</span>

e.g. Trigram:

```<s> <s> the teacher drinks tea </s>``` 

**Count Matrix & Probability Matrix**

$$\text{Note: } P\left( w_n \mid w_{n-N+1}^{n-1} \right) = \frac{ C\left(w_{n-N+1}^{n-1}, w_n\right)}{C\left( w_{n-N+1}^{n-1}\right)}, \text{w_n Ë°®Á§∫Á¨¨n‰∏™word}$$

- Count Matrix row: unique corpus (N-1)-grams
- Count Matrix columns: unique corpus words
- Probability Matrix: divde each cell from Count Matrix by Count Matrix row sum 
    - Count Matrix row sum equivalent to your counts of n-1 gram:  $$sum\left( row \right) = \sum_{w \in V} C\left(w_{n-N+1}^{n-1}, w\right) = C\left(w_{n-N+1}^{n-1}\right)$$

**Language Model**: to esitmate the probability of a given sentence

It estimate the probability by splitting sentence into series of n-grams then find their probability in the probability matrix. Then language model predict next elements of sequence by return the word with highest probability

**Log probability**:  all probability in calculation <= 1 and mutiplying them brings risk of underflow

![](/img/post/Natural-Language-Processing/course2/week3pic5.gif)

AlgorithmÔºö

1. Choose sentence start:  <span style="background-color: #FFFF00">**randomly choose** among all bigrams starting with ```<s>``` based on bigram probability. Bigrams with higher probabilities are more likely to be chosen.</span>
2. Choose next bigram starting with previous word from step 1 **randomly**, Ê¶ÇÁéáÊõ¥È´òÔºåË¢´ÊäΩÂà∞Ê¶ÇÁéáÊõ¥Â§ß
3. Continue until ```</s>``` is picked


e.g. :  choose ÊòØÁªøËâ≤

![](/img/post/Natural-Language-Processing/course2/week3pic6.png)


**Language Model Evaluation**

 - For smaller corpora: 80% Train, 10% Validation(used for tuning hyper-parameters), 10% Test
 - For large corpora(typical for text): 98% Train, 1% Validation(used for tuning hyper-parameters), 1% Test

![](/img/post/Natural-Language-Processing/course2/week3pic7.png)

**Perplexity** 

$$PP\left(W \right) = P\left(s_1,s_2,...,s_m \right)^{\frac{1}{m}}$$

- W : test set containing m sentence
- $$S_i$$ i-th sentence in the test set, each ending with ```</s>```
- m: number of all words in entire test set W <span style="color:red">including ```</s>``` but not including ```<s>```</span>
- Perplexity tells whether a set of sentences look like written by human or machine choosing words at random
- <span style="background-color: #FFFF00">A text written by human => low perplexity score. Text generated by random word choice => high perplexity score </span>
- Perplexity is the inverse probability of test sets normalized by <span style="color:red">the number of words</span> in the test set => higher the language model probability esitmates lower perplexity will be
- <span style="color:red">Perplexity is closely related to **entropy** which measures uncertainty</span>
- <span style="background-color: #FFFF00">Good language model have perplexity score between 60 to 20. Log perplexity is between 4.3 and 5.9 </span>
- perplexity for character level language lower perplxity for than word-based model
- The same probability for different test sets, the bigger m the lower final perplexity will be

e.g. 100 word in the text sets, m = 100, probability 0.9 which is high => means predict test sets very well

$$PP\left(W \right) = 0.9 => PP\left( W \right) = 0.9^{-\frac{1}{100}} = 1.00105416, \color{navy}{\text{very low perplexity}}$$

$$PP\left(W \right) = 10^{-250} => PP\left( W \right) =  \left(10^{-250} \right)^{-\frac{1}{100}} = 316, \color{navy}{\text{very high perplexity}}$$

**Perplexity for bigram model**

$$PP\left(W \right) = \sqrt[m]{\prod_{i=1}^{m} \prod_{j=1}^{ \mid s_i \mid} \frac{1}{ P\left( w_j^{\left(i\right)} \mid  w_{j-1}^{\left(i\right)} \right)   }}, \,\, \color{red}{w_j^{\left(i\right)}\text{: j-th word in i-th sentence}}$$

**If all sentences in the test cases are concatenated:**

$$PP\left(W \right) = \sqrt[m]{\prod_{i=1}^{m}  \frac{1}{ P\left( w_i \mid  w_{i-1} \right)   }}, \,\, \color{red}{w_i \text{: i-th word in test set}}$$

Use log perplexity insetad of perplexity

$$logPP\left(W \right) = -\frac{1}{m}\sum_{i=1}^{m}  log_2\left(P\left( w_i \mid  w_{i-1} \right)  \right), \,\, \color{red}{w_i \text{: i-th word in test set}}$$


**Out of vocabulary**

- closed vocabulary: a fixed last of word.   a set of unique word supported by a language model. e.g. chatbot only answer limited questions
- open vocabulary: deal with words not seen before (out of vocabulary)
- unknown word also **called out of vocabuarly word (OOV)**
- replace unknown word with special tag ```<UNK>``` in corpus and in input

Using ```<UNK>``` in corpus

1. create vocabulary V
   - Criteria 1: min word frequency f: Ëá≥Â∞ëÂú®corpus ‰∏≠Âá∫Áé∞fÂõû
   - Criteria 2: max size of vocabulary, only include words by frequency to the maximum vocabulary size
2. Replace any word in corpus and not in V by ```<UNK>```
3. Count the probabilities with ```<UNK>``` as with any other word, 
    - ```<UNK>``` usually lower perplexity

Note: 

- You might have lot of ```<UNK>```, then model generate a sequence of ```<UNK>``` quotes with high probability instead of meaningful sentences. Due to this limitation, using ```<UNK>``` sparingly
- <span style="background-color: #FFFF00">Only compare Language model perplexity with the same vocabulary</span>

ÊØîÂ¶Ç‰∏ãÈù¢‰æãÂ≠êÔºåvocabulary ÁöÑËØçÂøÖÈ°ªË¶ÅÂú®corpus‰∏≠Ëá≥Â∞ëÂá∫Áé∞‰∏§Âõû

![](/img/post/Natural-Language-Processing/course2/week3pic8.png)



**Smoothing**

Problem: N-gram made of known words still migh be missing in training corpus. ÊØîÂ¶Ç ```eat chocolate. John drinks```, bigram ```John eat``` are missing in bigram

![](/img/post/Natural-Language-Processing/course2/week3pic9.png)


**Add-one smooething (Laplacian smoothing)**: add one occurence to each bigram. bigram that are missing in the corpus have nonzero probability. add one in each cell in the Count Matrix


$$P\left( w_n \mid w_{n-1} \right) = \frac{ C\left(w_{n-1}, w_n\right) + 1}{\sum_{w \in V} C\left( w_{n-1}, W\right) + 1} = \frac{C\left(w_{n-1}, w_n\right) + 1}{C\left(w_{n-1}\right) + V}$$

Plus V only works if real counts are large enough to outweigh the plus one. Otherwise the probability of missing word will be too highÔºåÊØîÂ¶ÇÂ∞±vocubary size = 2, $$ P\left(UNK \mid known word \right) =  \frac{C\left(w_{n-1}, w_n\right) + 1}{C\left(w_{n-1}\right) + V} = \frac{1}{1 + 2} = \frac{1}{3}$$



**Add-k smooething**: make probability even smoother. can be applied to high order n-gram probabilities like trigram, four grams and beyond

$$P\left( w_n \mid w_{n-1} \right) = \frac{ C\left(w_{n-1}, w_n\right) + k}{\sum_{w \in V} C\left( w_{n-1}, W\right) + k} = \frac{C\left(w_{n-1}, w_n\right) + k}{C\left(w_{n-1}\right) + k*V}$$

some other advanced smoothing: Kneser-Ney smoothing, Good-Turing smoothing

e.g. Corpus "I am happy I am learning"  and k = 3, vocabulary size is 4 ```(I, am), (am, happy), (happy, I), (am, learning)```, ```(I, am)``` count is 2

$$P\left(can \mid I\right) = \frac{3}{2 + 3*4}$$


**Backoff**

- <span style="color:red">if N-gram missing => using (N-1) gram. if (N-1) gram missing, using (N-2) gram... so on until find nonzero probabilities</span>
- using lower level N-gram, distort probabilities especially for smaller corpora
  - Probability need to be discounted from high probability to lower probability e.g. *Katz backoff*
  - a large web-scale corpus : *"stupid" backoff*:  no probability discounting applied. If high order n-gram probability is missing, the lower order n-gram probability is used by multiplying a constant. <span style="color:red">0.4 is the constant expertimentally shown to work well</span>


![](/img/post/Natural-Language-Processing/course2/week3pic10.png)


**Linear Interpolation**: 

linear interpolation of all orders of n-gram: combined weighted probabilities of n-gram, (n-1) gram down to unigrams. <span style="background-color: #FFFF00">Lambda are learned from validation parts of the corpus. Can get them by maximizing the probability of sentences from the validation set</span>. The interpolation can be used for general n-gram by using more Lambdas


$$\hat P\left( w_n \mid w_{n-2}w_{n-1} \right) = \lambda_1 \times P\left( w_n \mid w_{n-2}w_{n-1} \right) +\lambda_2 \times P\left( w_n \mid w_{n-2} \right) + \lambda_3 \times P\left( w_n \right), given \sum_i \lambda_i = 1 $$

e.g. John drink chocolate

$$\hat P\left( chocolate \mid John \, drink \right) = 0.7 \times P\left( chocolate \mid John \, drink  \right) +0.2 \times P\left( chocolate \mid drink \right) + 0.1 \times P\left( chocolate \right) $$


#### Week4 Word Embeddings with Neural Networks

Build a vocabulary: assign integer to word, <span style="color:red"> Problem: order doesn't make sense from a semantic perspective. e.g. no reason that happy number > zebra</span> 

![](/img/post/Natural-Language-Processing/course2/week4pic1.png)


**one-hot vectors**: 

- put 1 in the row that has the same label and 0 everywhere else,
-  <span style="color:red"> Advantage: not imply any relationship between any two words</span>. e.g. Each vector says the word either happy or not , either zebra or not
-  Huge vectors: require a lot of space and processing time. If created with English word, may have 1 million rows, one row for each English word
-  No embedding meaning: if calculate the distance btween one-hot vectors, always get the same distance btween any two pairs of words. ```d(paper, excited) = d(paper, happy) = d(excited, happy)```, (inner product of any 2 one-hot vector is zero). ‰ΩÜÊòØintuitively, happy is more similar to excited than paper


e.g. each vector has 1000 elements,

![](/img/post/Natural-Language-Processing/course2/week4pic2.gif)

**Word Embedding**

ÊØîÂ¶ÇÊ®™ÂùêÊ†áÔºåword are positive right, word are negative left; Á∫µÂùêÊ†á word are concrete, higher. word are abstract lower. <span style="background-color: #FFFF00">Less precise than one hot vector</span>, ÂèØËÉΩÊúâ‰∏§‰∏™ÁÇπÈáçÂêàÔºåÊØîÂ¶Çsnake & spider


![](/img/post/Natural-Language-Processing/course2/week4pic3.png)

‰∏äÈù¢‰æãÂ≠êÊòØ‰∏Ä‰∏™word embedding. 

- Low dimension: Word embedding represent words in  a vector form that has a relative low dimension
- Carry embed meaning:  
  - e.g. semantic distance:  $$\text{forst} \approx \text{tree, forest} \not\approx \text{ticket}$$$
  - e.g. analgies :  Paris -> France similar as Rome -> Italy

![](/img/post/Natural-Language-Processing/course2/week4pic4.png)


**Create Word Embedding**

- If you want to generate word embedding based on Shakespeare, corpus should be full and original text of Shakespeare, not stude notes, slide presentation or keywords from Shakespeare. 
- <span style="background-color:#FFFF00">Context(combination of words to occur around that particular word) is important, give meaning to each word embedding</span>. a simple vocabulary list of Shakespeare's most common words not enough to create mebedding. 
- Objective of word embedding is to predict missing word
- Embedding method, machine learning models which are set to learn word embeddings. <span style="color:red">The meaning of words, as carred by word embeddings depends on the embedding approach</span>
  - <span style="background-color:#FFFF00">**Self-supervised learning**: Both unsupervised (input data are unlabeled) and supervised(text provide necessary context which would ordinarily make up the label)</span>
- Word embedding can be tuned by a number of hyperparameters. One hyperparameter is dimension of word embedding vectors. In practice, could from a few hundred to low thousand
  - <span style="color:red">Use high dimension captures more nuanced meanings but computatitional expensive</span>, may leading to diminishing return
- Corpus must first be transformed into a suitable mathematical representation. e.g. Integer word Indices or one-hot vector

![](/img/post/Natural-Language-Processing/course2/week4pic5.png)


- **word2vec**: (Google, 2013). use a shallowed neural network to learn word embedding. propose two model architectures: 
  - Continuous bag-of-words(CBOW): predict a missing word given the surrounding words
  - Continuous skip-gram also know as  Skip-gram with negative sampling (SGNS): did the reverse of Continuous bag-of-words:  model predict surrounding given a input word
- **Global Vectors(GloVe)** (Standford 2014): Involves factorizing the logarithm of the corpuses word co-occurrence matrix.(Similar to Count Matrix as above)
- **fastText** (Facebook 2016): based on skip-gram model and take into account the structure of words by representing words an n-gram of character
  - support out-of-vocabulary(OOV) words: infer embedding from the sequence of chracters they are made of and corresponding sequence that it was intially trained on. E.g. create similar embedding for kitty and kitten even kitty never seen before(Âõ†‰∏∫ kitty and kitten has similar sequences of characters)
  - another benefit: embedding vectors can be averaged together to make vector representations of phrases and sentences
- Advanced work embedding methods: use advanced deep neural network to refine word meanings according  to their contexts. ‰∏ÄËØçÂ§ö‰πâ(polysemy or words with similar meanings), ‰πãÂâçmodel  always has the same embedding e.g. plants. ÂèØ‰ª•ÊòØ organism like a flower, a factory or adverb. below are Tunable pre-trained model available
  - BERT (Google, 2018): Bidirectional representations from transformers by Google
  - ELMo (Allen Institute for AI, 2018)P embeddings from language models
  - GPT-2 (OpenAI,2018): generative pre-training 2


**Continuous bag-of-words**

- <span style="background-color:#FFFF00">the objective: predict a missing word based on the surrounding words</span>
- rationale: if two unique words are both frequently surrounded by a similar set of words in various sentences, then Ëøô‰∏§‰∏™ËØç tend to related in their meaning(related semantically)

![](/img/post/Natural-Language-Processing/course2/week4pic6.png)

- define <span style="background-color:#FFFF00">**context words**</span> as four words. C= 2 <span style="color:red">Two words before center word and two words after it</span>. <span style="background-color:#FFFF00">C is the hyperparameter of the model</span>
- Window: center word + context words
- Architecture: context words as input and center words as output

![](/img/post/Natural-Language-Processing/course2/week4pic7.gif)


**Tokenization**

- Case insensitive: The==the==THE  -> conver to either all lowercase or all uppercase
- Punctuation  
  - interruption punctuation marks ```,!.?``` as single special word of the vocabulary.  
  - ignore non interrupting punctuation marks ```'<< >>"```
  - collapse multi-sign marks into single marks ```!!!``` -> ```.``` , ```...``` -> ```.```
- Number: 
  - If numbers do not carry important meaning -> drop
  - number may have meaning. e.g. 3,1415926 -> pi, 90210 is name of television show and zip code => keep as ```/<NUMBER>```
- Special characters: such as mathematical symbols, currency symbols, section and paragraph signs and line markup signs => drop
- Special words: emojis, hastags (like twitter) => depend on if and how handle 
  - can consider each emoji or hashtag considered as individual word. üòÑ  -> ```:happy```


```python
# pip install nltk

#pip install moji

import nltk
from nltk.tokenize import word_tokenize
import emoji

nltk.download('punkt') #download pre-trained Punkt tokenizer for English

# handle common special uses of punctuation

```
‰∏ãÈù¢‰æãÂ≠ê, question marks and exclamation convert to a single full stop, last step get rid of number but keep alphabetical, full stops(previously an interrupting punctuation mark), emoji

![](/img/post/Natural-Language-Processing/course2/week4pic8.gif)


![](/img/post/Natural-Language-Processing/course2/week4pic9.gif)


**Transforming Words Into Vectors**

Corpus: I am happy because I am learning</br>
Vocabulary: am, because, happy, I, learning  (vocabulary is the set of unique set)


- Center word:  One-hot vector: ‰∏ãÈù¢‰æãÂ≠êorder by alphabetic orderÔºåÂÆûÈôÖ‰∏≠ÂèØ‰ª•ÊòØarbitrary 
- Context word: take the average of one-hot vectors of each context word

![](/img/post/Natural-Language-Processing/course2/week4pic10.gif)